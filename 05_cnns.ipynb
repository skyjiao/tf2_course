{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you will learn how to build Convolutional Neural Networks (CNNs) for image processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python 3.7.3 (default, Apr  3 2019, 05:39:12) \n",
      "[GCC 8.3.0]\n",
      "matplotlib 3.1.0\n",
      "numpy 1.16.4\n",
      "pandas 0.24.2\n",
      "sklearn 0.21.2\n",
      "tensorflow 2.0.0-dev20190623\n",
      "tensorflow.python.keras.api._v2.keras 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "print(\"python\", sys.version)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sys.version_info >= (3, 5) # Python ≥3.5 required\n",
    "assert tf.__version__ >= \"2.0\"    # TensorFlow ≥2.0 required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Exercise](https://c1.staticflickr.com/9/8101/8553474140_c50cf08708_b.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 – Simple CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1)\n",
    "Load CIFAR10 using `keras.datasets.cifar10.load_data()`, and split it into a training set (45,000 images), a validation set (5,000 images) and a test set (10,000 images). Make sure the pixel values range from 0 to 1. Visualize a few images using `plt.imshow()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "    \"airplane\",\n",
    "    \"automobile\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_all, y_train_all), (X_test, y_test) = keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_all, y_train_all, test_size=5000, random_state=42)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "nb_pixels = 32\n",
    "X_train_scaled = scaler.fit_transform(X_train.astype(np.float32).reshape(-1, nb_pixels * nb_pixels * 3)).reshape(-1, nb_pixels, nb_pixels, 3)\n",
    "X_valid_scaled = scaler.transform(X_valid.astype(np.float32).reshape(-1, nb_pixels * nb_pixels * 3)).reshape(-1, nb_pixels, nb_pixels, 3)\n",
    "X_test_scaled = scaler.transform(X_test.astype(np.float32).reshape(-1, nb_pixels * nb_pixels * 3)).reshape(-1, nb_pixels, nb_pixels, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "X_train = X_train_full[:-5000] / 255\n",
    "y_train = y_train_full[:-5000]\n",
    "X_valid = X_train_full[-5000:] / 255\n",
    "y_valid = y_train_full[-5000:]\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45000, 32, 32, 3)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0624 17:14:02.610169 140150176913216 image.py:693] Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f76ae8c2e80>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train_scaled[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({3: 4477,\n",
       "         1: 4513,\n",
       "         0: 4524,\n",
       "         2: 4473,\n",
       "         4: 4514,\n",
       "         5: 4496,\n",
       "         9: 4500,\n",
       "         7: 4491,\n",
       "         6: 4515,\n",
       "         8: 4497})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(y_train.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2)\n",
    "Build and train a baseline model with a few dense layers, and plot the learning curves. Use the model's `summary()` method to count the number of parameters in this model.\n",
    "\n",
    "**Tip**:\n",
    "\n",
    "* Recall that to plot the learning curves, you can simply create a Pandas `DataFrame` with the `history.history` dict, then call its `plot()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.models.Sequential(name=\"base_model\")\n",
    "base_model.add(keras.layers.Flatten(input_shape=[32,32,3], name=\"flatten_layer\"))\n",
    "base_model.add(keras.layers.Dense(100, activation=\"selu\",kernel_initializer=\"lecun_normal\",name=\"dense_layer_1\"))\n",
    "base_model.add(keras.layers.Dense(50, activation=\"selu\", kernel_initializer=\"lecun_normal\", name=\"dense_layer_2\"))\n",
    "base_model.add(keras.layers.Dense(10, activation=\"softmax\", name=\"output_layer\"))\n",
    "base_model.compile(optimizer=keras.optimizers.Adam(learning_rate = 1e-3), \n",
    "              loss=\"sparse_categorical_crossentropy\", \n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"base_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_layer (Flatten)      (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_layer_1 (Dense)        (None, 100)               307300    \n",
      "_________________________________________________________________\n",
      "dense_layer_2 (Dense)        (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 312,860\n",
      "Trainable params: 312,860\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "45000/45000 [==============================] - 3s 70us/sample - loss: 1.0880 - accuracy: 0.6108 - val_loss: 1.4743 - val_accuracy: 0.4960\n",
      "Epoch 2/100\n",
      "45000/45000 [==============================] - 3s 70us/sample - loss: 1.0582 - accuracy: 0.6201 - val_loss: 1.5257 - val_accuracy: 0.4914\n",
      "Epoch 3/100\n",
      "45000/45000 [==============================] - 3s 70us/sample - loss: 1.0285 - accuracy: 0.6303 - val_loss: 1.5259 - val_accuracy: 0.4960\n",
      "Epoch 4/100\n",
      "45000/45000 [==============================] - 3s 71us/sample - loss: 1.0130 - accuracy: 0.6368 - val_loss: 1.5464 - val_accuracy: 0.4938\n",
      "Epoch 5/100\n",
      "45000/45000 [==============================] - 3s 71us/sample - loss: 0.9828 - accuracy: 0.6481 - val_loss: 1.6077 - val_accuracy: 0.4922\n",
      "Epoch 6/100\n",
      "45000/45000 [==============================] - 3s 71us/sample - loss: 0.9566 - accuracy: 0.6591 - val_loss: 1.5933 - val_accuracy: 0.4982\n"
     ]
    }
   ],
   "source": [
    "history = base_model.fit(X_train_scaled, y_train, epochs=100, validation_data=(X_valid_scaled, y_valid), \n",
    "                    callbacks = [tf.keras.callbacks.EarlyStopping(patience=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3)\n",
    "Build and train a Convolutional Neural Network using a \"classical\" architecture: N * (Conv2D → Conv2D → Pool2D) → Flatten → Dense → Dense. Before you print the `summary()`, try to manually calculate the number of parameters in your model's architecture, as well as the shape of the inputs and outputs of each layer. Next, plot the learning curves and compare the performance with the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = keras.models.Sequential(name = \"cnn_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.add(tf.keras.layers.InputLayer(input_shape = (32, 32, 3)))\n",
    "cnn_model.add(tf.keras.layers.Conv2D(filters = 64, \n",
    "                                     kernel_size=(3, 3), \n",
    "                                     padding='same', \n",
    "                                     activation = \"relu\"\n",
    "                                     ))\n",
    "cnn_model.add(tf.keras.layers.Conv2D(filters = 128, \n",
    "                                     kernel_size=(3, 3), \n",
    "                                     padding='same', \n",
    "                                     activation = \"relu\"))\n",
    "cnn_model.add(tf.keras.layers.MaxPool2D())\n",
    "cnn_model.add(tf.keras.layers.Flatten())\n",
    "cnn_model.add(tf.keras.layers.Dense(128, activation=\"relu\"))\n",
    "cnn_model.add(tf.keras.layers.Dense(10, activation=\"softmax\", name=\"output_layer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.compile(optimizer=keras.optimizers.Adam(learning_rate = 1e-3), \n",
    "                  loss=\"sparse_categorical_crossentropy\", \n",
    "                  metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "45000/45000 [==============================] - 150s 3ms/sample - loss: 1.3674 - accuracy: 0.5113 - val_loss: 1.0781 - val_accuracy: 0.6242\n",
      "Epoch 2/100\n",
      " 7360/45000 [===>..........................] - ETA: 2:09 - loss: 1.0059 - accuracy: 0.6473"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-2a035dcfef51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                         callbacks = [tf.keras.callbacks.EarlyStopping(patience=5)])\n\u001b[0m",
      "\u001b[0;32m~/Projects/perso/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/Projects/perso/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/Projects/perso/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/perso/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3481\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3482\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3483\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3485\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/perso/venv/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    581\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m    582\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/perso/venv/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m    683\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/perso/venv/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    452\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    453\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 454\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    455\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/perso/venv/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = cnn_model.fit(X_train, \n",
    "                        y_train, \n",
    "                        epochs=100, \n",
    "                        validation_data=(X_valid, y_valid),\n",
    "                        callbacks = [tf.keras.callbacks.EarlyStopping(patience=5)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4)\n",
    "Looking at the learning curves, you can see that the model is overfitting. Add a Batch Normalization layer after each convolutional layer. Compare the model's performance and learning curves with the previous model.\n",
    "\n",
    "**Tip**: there is no need for an activation function just before the pooling layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Exercise solution](https://camo.githubusercontent.com/250388fde3fac9135ead9471733ee28e049f7a37/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f302f30362f46696c6f735f736567756e646f5f6c6f676f5f253238666c69707065642532392e6a7067)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 – Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1)\n",
    "Load CIFAR10 using `keras.datasets.cifar10.load_data()`, and split it into a training set (45,000 images), a validation set (5,000 images) and a test set (10,000 images). Make sure the pixel values range from 0 to 1. Visualize a few images using `plt.imshow()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "    \"airplane\",\n",
    "    \"automobile\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "X_train = X_train_full[:-5000] / 255\n",
    "y_train = y_train_full[:-5000]\n",
    "X_valid = X_train_full[-5000:] / 255\n",
    "y_valid = y_train_full[-5000:]\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "n_rows, n_cols = 10, 15\n",
    "for row in range(n_rows):\n",
    "    for col in range(n_cols):\n",
    "        i = row * n_cols + col\n",
    "        plt.subplot(n_rows, n_cols, i + 1)\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(X_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the classes of the images in the first row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_cols):\n",
    "    print(classes[y_train[i][0]], end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2)\n",
    "Build and train a baseline model with a few dense layers, and plot the learning curves. Use the model's `summary()` method to count the number of parameters in this model.\n",
    "\n",
    "**Tip**:\n",
    "\n",
    "* Recall that to plot the learning curves, you can simply create a Pandas `DataFrame` with the `history.history` dict, then call its `plot()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[32, 32, 3]),\n",
    "    keras.layers.Dense(64, activation=\"selu\"),\n",
    "    keras.layers.Dense(64, activation=\"selu\"),\n",
    "    keras.layers.Dense(64, activation=\"selu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=0.01), metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot()\n",
    "plt.axis([0, 19, 0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3)\n",
    "Build and train a Convolutional Neural Network using a \"classical\" architecture: N * (Conv2D → Conv2D → Pool2D) → Flatten → Dense → Dense. Before you print the `summary()`, try to manually calculate the number of parameters in your model's architecture, as well as the shape of the inputs and outputs of each layer. Next, plot the learning curves and compare the performance with the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\", input_shape=[32, 32, 3]),\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.MaxPool2D(pool_size=2),\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.MaxPool2D(pool_size=2),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=0.01), metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot()\n",
    "plt.axis([0, 19, 0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of params in a convolutional layer =\n",
    "# (kernel_width * kernel_height * channels_in + 1 for bias) * channels_out\n",
    "(\n",
    "    (3 * 3 * 3 + 1)  * 32  # in: 32x32x3   out: 32x32x32  Conv2D\n",
    "  + (3 * 3 * 32 + 1) * 32  # in: 32x32x32  out: 32x32x32  Conv2D\n",
    "  + 0                      # in: 32x32x32  out: 16x16x32  MaxPool2D\n",
    "  + (3 * 3 * 32 + 1) * 64  # in: 16x16x32  out: 16x16x64  Conv2D\n",
    "  + (3 * 3 * 64 + 1) * 64  # in: 16x16x64  out: 16x16x64  Conv2D\n",
    "  + 0                      # in: 16x16x64  out: 8x8x64    MaxPool2D\n",
    "  + 0                      # in: 8x8x64    out: 4096      Flatten\n",
    "  + (4096 + 1) * 128       # in: 4096      out: 128       Dense\n",
    "  + (128 + 1) * 10         # in: 128       out: 10        Dense\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4)\n",
    "Looking at the learning curves, you can see that the model is overfitting. Add a Batch Normalization layer after each convolutional layer. Compare the model's performance and learning curves with the previous model.\n",
    "\n",
    "**Tip**: there is no need for an activation function just before the pooling layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\", input_shape=[32, 32, 3]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=2),\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=2),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=0.01), metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot()\n",
    "plt.axis([0, 19, 0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Exercise](https://c1.staticflickr.com/9/8101/8553474140_c50cf08708_b.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 – Separable Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1)\n",
    "Replace the `Conv2D` layers with `SeparableConv2D` layers (except the first one), fit your model and compare its performance and learning curves with the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2)\n",
    "Try to estimate the number of parameters in your network, then check your result with `model.summary()`.\n",
    "\n",
    "**Tip**: the batch normalization layer adds two parameters for each feature map (the scale and bias)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Exercise solution](https://camo.githubusercontent.com/250388fde3fac9135ead9471733ee28e049f7a37/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f302f30362f46696c6f735f736567756e646f5f6c6f676f5f253238666c69707065642532392e6a7067)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 – Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1)\n",
    "Replace the `Conv2D` layers with `SeparableConv2D` layers (except the first one), fit your model and compare its performance and learning curves with the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\", input_shape=[32, 32, 3]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.SeparableConv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=2),\n",
    "    keras.layers.SeparableConv2D(filters=64, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.SeparableConv2D(filters=64, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=2),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=0.01), metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot()\n",
    "plt.axis([0, 19, 0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2)\n",
    "Try to estimate the number of parameters in your network, then check your result with `model.summary()`.\n",
    "\n",
    "**Tip**: the batch normalization layer adds two parameters for each feature map (the scale and bias)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of params in a depthwise separable 2D convolution layer =\n",
    "# kernel_width * kernel_height * channels_in + (channels_in + 1 for bias) * channels_out\n",
    "(\n",
    "    (3 * 3 * 3 + 1) * 32        # in: 32x32x3   out: 32x32x32  Conv2D\n",
    "  + 32 * 2                      # in: 32x32x32  out: 32x32x32  BN\n",
    "  + 3 * 3 * 32 + (32 + 1) * 32  # in: 32x32x32  out: 32x32x32  SeparableConv2D\n",
    "  + 32 * 2                      # in: 32x32x32  out: 32x32x32  BN\n",
    "  + 0                           # in: 32x32x32  out: 16x16x32  MaxPool2D\n",
    "  + 3 * 3 * 32 + (32 + 1) * 64  # in: 16x16x32  out: 16x16x64  SeparableConv2D\n",
    "  + 64 * 2                      # in: 16x16x64  out: 16x16x64  BN\n",
    "  + 3 * 3 * 64 + (64 + 1) * 64  # in: 16x16x64  out: 16x16x64  SeparableConv2D\n",
    "  + 64 * 2                      # in: 16x16x64  out: 16x16x64  BN\n",
    "  + 0                           # in: 16x16x64  out: 8x8x64    MaxPool2D\n",
    "  + 0                           # in: 8x8x64    out: 4096      Flatten\n",
    "  + (4096 + 1) * 128            # in: 4096      out: 128       Dense\n",
    "  + (128 + 1) * 10              # in: 128       out: 10        Dense\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Exercise](https://c1.staticflickr.com/9/8101/8553474140_c50cf08708_b.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 – Pretrained CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1)\n",
    "Using `keras.preprocessing.image.load_img()` followed by `keras.preprocessing.image.img_to_array()`, load one or more images (e.g., `fig.jpg` or `ostrich.jpg` in the `images` folder). You should set `target_size=(299, 299)` when calling `load_img()`, as this is the shape that the Xception network expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2)\n",
    "Create a batch containing the image(s) you just loaded, and preprocess this batch using `keras.applications.xception.preprocess_input()`. Verify that the features now vary from -1 to 1: this is what the Xception network expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3)\n",
    "Create an instance of the Xception model (`keras.applications.xception.Xception`) and use its `predict()` method to classify the images in the batch. You can use `keras.applications.resnet50.decode_predictions()` to convert the output matrix into a list of top-N predictions (with their corresponding class labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Exercise solution](https://camo.githubusercontent.com/250388fde3fac9135ead9471733ee28e049f7a37/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f302f30362f46696c6f735f736567756e646f5f6c6f676f5f253238666c69707065642532392e6a7067)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 – Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1)\n",
    "Using `keras.preprocessing.image.load_img()` followed by `keras.preprocessing.image.img_to_array()`, load one or more images (e.g., `fig.jpg` or `ostrich.jpg` in the `images` folder). You should set `target_size=(299, 299)` when calling `load_img()`, as this is the shape that the Xception network expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_fig_path = os.path.join(\"images\", \"fig.jpg\")\n",
    "img_fig = keras.preprocessing.image.load_img(img_fig_path, target_size=(299, 299))\n",
    "img_fig = keras.preprocessing.image.img_to_array(img_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(img_fig / 255)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "img_fig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ostrich_path = os.path.join(\"images\", \"ostrich.jpg\")\n",
    "img_ostrich = keras.preprocessing.image.load_img(img_ostrich_path, target_size=(299, 299))\n",
    "img_ostrich = keras.preprocessing.image.img_to_array(img_ostrich)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_ostrich / 255)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "img_ostrich.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2)\n",
    "Create a batch containing the image(s) you just loaded, and preprocess this batch using `keras.applications.xception.preprocess_input()`. Verify that the features now vary from -1 to 1: this is what the Xception network expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch = np.array([img_fig, img_ostrich])\n",
    "X_preproc = keras.applications.xception.preprocess_input(X_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_preproc.min(), X_preproc.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3)\n",
    "Create an instance of the Xception model (`keras.applications.xception.Xception`) and use its `predict()` method to classify the images in the batch. You can use `keras.applications.resnet50.decode_predictions()` to convert the output matrix into a list of top-N predictions (with their corresponding class labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.applications.xception.Xception()\n",
    "Y_proba = model.predict(X_preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_proba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(Y_proba, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_predictions = keras.applications.resnet50.decode_predictions(Y_proba)\n",
    "for preds in decoded_predictions:\n",
    "    for wordnet_id, name, proba in preds:\n",
    "        print(\"{} ({}): {:.1f}%\".format(name, wordnet_id, 100 * proba))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Exercise](https://c1.staticflickr.com/9/8101/8553474140_c50cf08708_b.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 – Data Augmentation and Transfer Learning\n",
    "In this exercise you will reuse a pretrained Xception model to build a flower classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's download the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "\n",
    "flowers_url = \"http://download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "flowers_path = keras.utils.get_file(\"flowers.tgz\", flowers_url, extract=True)\n",
    "flowers_dir = os.path.join(os.path.dirname(flowers_path), \"flower_photos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, subdirs, files in os.walk(flowers_dir):\n",
    "    print(root)\n",
    "    for filename in files[:3]:\n",
    "        print(\"   \", filename)\n",
    "    if len(files) > 3:\n",
    "        print(\"    ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1)\n",
    "Build a `keras.preprocessing.image.ImageDataGenerator` that will preprocess the images and do some data augmentation (the [documentation](https://keras.io/preprocessing/image/) contains useful examples):\n",
    "\n",
    "* It should at least perform horizontal flips and keep 10% of the data for validation, but you may also make it perform a bit of rotation, rescaling, etc.\n",
    "* Also make sure to apply the Xception preprocessing function (using the `preprocessing_function` argument).\n",
    "* Call this generator's `flow_from_directory()` method to get an iterator that will load and preprocess the flower photos from the `flower_photos` directory, setting the target size to (299, 299) and `subset` to `\"training\"`.\n",
    "* Call this method again with the same parameters except `subset=\"validation\"` to get a second iterator for validation.\n",
    "* Get the next batch from the validation iterator and display the first image from the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2)\n",
    "Now let's build the model:\n",
    "* Create a new `Xception` model, but this time set `include_top=False` to get the model without the top layer. **Tip**: you will need to access its `input` and `output` properties.\n",
    "* Make all its layers non-trainable.\n",
    "* Using the functional API, add a `GlobalAveragePooling2D` layer (feeding it the Xception model's output), and add a `Dense` layer with 5 neurons and the Softmax activation function.\n",
    "* Compile the model. **Tip**: don't forget to add the `\"accuracy\"` metric.\n",
    "* Fit your model using `fit_generator()`, passing it the training and validation iterators (and setting `steps_per_epoch` and `validation_steps` appropriately)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Exercise solution](https://camo.githubusercontent.com/250388fde3fac9135ead9471733ee28e049f7a37/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f302f30362f46696c6f735f736567756e646f5f6c6f676f5f253238666c69707065642532392e6a7067)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 – Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1)\n",
    "Build a `keras.preprocessing.image.ImageDataGenerator` that will preprocess the images and do some data augmentation (the [documentation](https://keras.io/preprocessing/image/) contains useful examples):\n",
    "\n",
    "* It should at least perform horizontal flips and keep 10% of the data for validation, but you may also make it perform a bit of rotation, rescaling, etc.\n",
    "* Also make sure to apply the Xception preprocessing function (using the `preprocessing_function` argument).\n",
    "* Call this generator's `flow_from_directory()` method to get an iterator that will load and preprocess the flower photos from the `flower_photos` directory, setting the target size to (299, 299) and `subset` to `\"training\"`.\n",
    "* Call this method again with the same parameters except `subset=\"validation\"` to get a second iterator for validation.\n",
    "* Get the next batch from the validation iterator and display the first image from the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.1,\n",
    "    preprocessing_function=keras.applications.xception.preprocess_input)\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "        flowers_dir,\n",
    "        target_size=(299, 299),\n",
    "        batch_size=32,\n",
    "        subset=\"training\")\n",
    "\n",
    "valid_generator = datagen.flow_from_directory(\n",
    "        flowers_dir,\n",
    "        target_size=(299, 299),\n",
    "        batch_size=32,\n",
    "        subset=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch, y_batch = next(valid_generator)\n",
    "plt.imshow((X_batch[0] + 1)/2)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2)\n",
    "Now let's build the model:\n",
    "* Create a new `Xception` model, but this time set `include_top=False` to get the model without the top layer. **Tip**: you will need to access its `input` and `output` properties.\n",
    "* Make all its layers non-trainable.\n",
    "* Using the functional API, add a `GlobalAveragePooling2D` layer (feeding it the Xception model's output), and add a `Dense` layer with 5 neurons and the Softmax activation function.\n",
    "* Compile the model. **Tip**: don't forget to add the `\"accuracy\"` metric.\n",
    "* Fit your model using `fit_generator()`, passing it the training and validation iterators (and setting `steps_per_epoch` and `validation_steps` appropriately)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 5\n",
    "\n",
    "base_model = keras.applications.xception.Xception(include_top=False)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "global_pool = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "predictions = keras.layers.Dense(n_classes, activation='softmax')(global_pool)\n",
    "\n",
    "model = keras.models.Model(base_model.input, predictions)\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"sgd\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=3306 // 32,\n",
    "    epochs=50,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=364 // 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot()\n",
    "plt.axis([0, 19, 0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Exercise](https://c1.staticflickr.com/9/8101/8553474140_c50cf08708_b.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Detection Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Google [Street View House Numbers](http://ufldl.stanford.edu/housenumbers/) (SVHN) dataset contains pictures of digits in all shapes and colors, taken by the Google Street View cars. The goal is to classify and locate all the digits in large images.\n",
    "* Train a Fully Convolutional Network on the 32x32 images.\n",
    "* Use this FCN to build a digit detector in the large images."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
