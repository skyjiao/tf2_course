{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low-Level TensorFlow API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you will learn how to use TensorFlow's low-level API, then use it to build custom loss functions, as well as custom Keras layers and models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python 3.7.3 (default, Apr  3 2019, 05:39:12) \n",
      "[GCC 8.3.0]\n",
      "matplotlib 3.1.0\n",
      "numpy 1.16.4\n",
      "pandas 0.24.2\n",
      "sklearn 0.21.2\n",
      "tensorflow 2.0.0-dev20190623\n",
      "tensorflow_core.keras 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "print(\"python\", sys.version)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sys.version_info >= (3, 5) # Python ≥3.5 required\n",
    "assert tf.__version__ >= \"2.0\"    # TensorFlow ≥2.0 required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors and operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can browse through the code examples or jump directly to the exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=339, shape=(2, 3), dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.constant([[1., 2., 3.], [4., 5., 6.]])\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.float32"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=344, shape=(2, 2), dtype=float32, numpy=\n",
       "array([[2., 3.],\n",
       "       [5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=349, shape=(2, 1), dtype=float32, numpy=\n",
       "array([[2.],\n",
       "       [5.]], dtype=float32)>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[..., 1, tf.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=352, shape=(2, 3), dtype=float32, numpy=\n",
       "array([[11., 12., 13.],\n",
       "       [14., 15., 16.]], dtype=float32)>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=354, shape=(2, 3), dtype=float32, numpy=\n",
       "array([[ 1.,  4.,  9.],\n",
       "       [16., 25., 36.]], dtype=float32)>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=358, shape=(2, 2), dtype=float32, numpy=\n",
       "array([[14., 32.],\n",
       "       [32., 77.]], dtype=float32)>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t @ tf.transpose(t) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To/From NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=361, shape=(2, 3), dtype=float64, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]])>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1., 2., 3.], [4., 5., 6.]])\n",
    "tf.constant(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  4.,  9.],\n",
       "       [16., 25., 36.]], dtype=float32)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.constant([[1., 2., 3.], [4., 5., 6.]])\n",
    "np.square(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=364, shape=(), dtype=float32, numpy=2.718>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.constant(2.718)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.718"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conflicting Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot compute Add as input #1(zero-based) was expected to be a int32 tensor but is a float tensor [Op:Add] name: add/\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tf.constant(1) + tf.constant(1.0)\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot compute Add as input #1(zero-based) was expected to be a double tensor but is a float tensor [Op:Add] name: add/\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tf.constant(1.0, dtype=tf.float64) + tf.constant(1.0)\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=374, shape=(), dtype=float32, numpy=2.0>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.constant(1.0, dtype=tf.float64)\n",
    "tf.cast(t, tf.float32) + tf.constant(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=376, shape=(), dtype=string, numpy=b'caf\\xc3\\xa9'>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.constant(\"café\")\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=378, shape=(), dtype=int32, numpy=5>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.length(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=380, shape=(), dtype=int32, numpy=4>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.length(t, unit=\"UTF8_CHAR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=385, shape=(4,), dtype=int32, numpy=array([ 99,  97, 102, 233], dtype=int32)>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.unicode_decode(t, \"UTF8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tf.constant([\"Café\", \"Coffee\", \"caffè\", \"咖啡\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=388, shape=(4,), dtype=int32, numpy=array([4, 6, 5, 2], dtype=int32)>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.length(t, unit=\"UTF8_CHAR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[67, 97, 102, 233], [67, 111, 102, 102, 101, 101], [99, 97, 102, 102, 232], [21654, 21857]]>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = tf.strings.unicode_decode(t, \"UTF8\")\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ragged tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[11, 12], [21, 22, 23], [], [41]]>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = tf.ragged.constant([[11, 12], [21, 22, 23], [], [41]])\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[11, 12], [21, 22, 23], [], [41]]>\n"
     ]
    }
   ],
   "source": [
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([21 22 23], shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(r[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[21, 22, 23]]>\n"
     ]
    }
   ],
   "source": [
    "print(r[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[11, 12], [21, 22, 23], [], [41], [51, 52], [], [71]]>\n"
     ]
    }
   ],
   "source": [
    "r2 = tf.ragged.constant([[51, 52], [], [71]])\n",
    "print(tf.concat([r, r2], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[11, 12, 13, 14, 15], [21, 22, 23, 24], [], [41, 42, 43]]>\n"
     ]
    }
   ],
   "source": [
    "r3 = tf.ragged.constant([[13, 14, 15], [24], [], [42, 43]])\n",
    "print(tf.concat([r, r3], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=619, shape=(4, 3), dtype=int32, numpy=\n",
       "array([[11, 12,  0],\n",
       "       [21, 22, 23],\n",
       "       [ 0,  0,  0],\n",
       "       [41,  0,  0]], dtype=int32)>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.to_tensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparseTensor(indices=tf.Tensor(\n",
      "[[0 1]\n",
      " [1 0]\n",
      " [2 3]], shape=(3, 2), dtype=int64), values=tf.Tensor([1. 2. 3.], shape=(3,), dtype=float32), dense_shape=tf.Tensor([3 4], shape=(2,), dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "s = tf.SparseTensor(indices=[[0, 1], [1, 0], [2, 3]],\n",
    "                    values=[1., 2., 3.],\n",
    "                    dense_shape=[3, 4])\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=628, shape=(3, 4), dtype=float32, numpy=\n",
       "array([[0., 1., 0., 0.],\n",
       "       [2., 0., 0., 0.],\n",
       "       [0., 0., 0., 3.]], dtype=float32)>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sparse.to_dense(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = s * 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsupported operand type(s) for +: 'SparseTensor' and 'float'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    s3 = s + 1.\n",
    "except TypeError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=633, shape=(3, 2), dtype=float32, numpy=\n",
       "array([[ 30.,  40.],\n",
       "       [ 20.,  40.],\n",
       "       [210., 240.]], dtype=float32)>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s4 = tf.constant([[10., 20.], [30., 40.], [50., 60.], [70., 80.]])\n",
    "tf.sparse.sparse_dense_matmul(s, s4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparseTensor(indices=tf.Tensor(\n",
      "[[0 2]\n",
      " [0 1]], shape=(2, 2), dtype=int64), values=tf.Tensor([1. 2.], shape=(2,), dtype=float32), dense_shape=tf.Tensor([3 4], shape=(2,), dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "s5 = tf.SparseTensor(indices=[[0, 2], [0, 1]],\n",
    "                     values=[1., 2.],\n",
    "                     dense_shape=[3, 4])\n",
    "print(s5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indices[1] = [0,1] is out of order [Op:SparseToDense]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tf.sparse.to_dense(s5)\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=648, shape=(3, 4), dtype=float32, numpy=\n",
       "array([[0., 2., 1., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s6 = tf.sparse.reorder(s5)\n",
    "tf.sparse.to_dense(s6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = tf.Variable([[1., 2., 3.], [4., 5., 6.]])\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=660, shape=(2, 3), dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 2.,  4.,  6.],\n",
       "       [ 8., 10., 12.]], dtype=float32)>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.assign(2 * v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 2., 42.,  6.],\n",
       "       [ 8., 10., 12.]], dtype=float32)>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[0, 1].assign(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 2., 42.,  6.],\n",
       "       [ 7.,  8.,  9.]], dtype=float32)>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[1].assign([7., 8., 9.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ResourceVariable' object does not support item assignment\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    v[1] = [7., 8., 9.]\n",
    "except TypeError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[4., 5., 6.],\n",
       "       [1., 2., 3.]], dtype=float32)>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_delta = tf.IndexedSlices(values=[[1., 2., 3.], [4., 5., 6.]],\n",
    "                                indices=[1, 0])\n",
    "v.scatter_update(sparse_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[100.,   5.,   6.],\n",
       "       [  1.,   2., 200.]], dtype=float32)>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.scatter_nd_update(indices=[[0, 0], [1, 2]],\n",
    "                    updates=[100., 200.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/cpu:0\"):\n",
    "    t = tf.constant([[1., 2., 3.], [4., 5., 6.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/job:localhost/replica:0/task:0/device:CPU:0'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.test.is_gpu_available():\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        t2 = tf.constant([[1., 2., 3.], [4., 5., 6.]])\n",
    "    print(t2.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Exercise](https://c1.staticflickr.com/9/8101/8553474140_c50cf08708_b.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 – Custom loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by loading and preparing the California housing dataset. We first load it, then split it into a training set, a validation set and a test set, and finally we scale it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1)\n",
    "Create an `my_mse()` function with two arguments: the true labels `y_true` and the model predictions `y_pred`. Make it return the mean squared error using TensorFlow operations. Note that you could write your own custom metrics in exactly the same way. **Tip**: recall that the MSE is the mean of the squares of prediction errors, which are the differences between the predictions and the labels, so you will need to use `tf.reduce_mean()` and `tf.square()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_mse(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2)\n",
    "Compile the following model, passing it your custom loss function, then train it and evaluate it. **Tip**: don't forget to use the scaled sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "11610/11610 [==============================] - 0s 33us/sample - loss: 2.0090 - val_loss: 0.7489\n",
      "Epoch 2/100\n",
      "11610/11610 [==============================] - 0s 26us/sample - loss: 0.6035 - val_loss: 0.5347\n",
      "Epoch 3/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.4862 - val_loss: 0.4771\n",
      "Epoch 4/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.4393 - val_loss: 0.3924\n",
      "Epoch 5/100\n",
      "11610/11610 [==============================] - 0s 26us/sample - loss: 0.4141 - val_loss: 0.6198\n",
      "Epoch 6/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.4009 - val_loss: 0.3693\n",
      "Epoch 7/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3917 - val_loss: 0.4321\n",
      "Epoch 8/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3855 - val_loss: 0.3674\n",
      "Epoch 9/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3794 - val_loss: 0.6249\n",
      "Epoch 10/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3812 - val_loss: 0.5782\n",
      "Epoch 11/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3748 - val_loss: 1.1442\n",
      "Epoch 12/100\n",
      "11610/11610 [==============================] - 0s 26us/sample - loss: 0.3762 - val_loss: 0.8963\n",
      "Epoch 13/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3708 - val_loss: 0.7359\n",
      "Epoch 14/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3673 - val_loss: 0.6451\n",
      "Epoch 15/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3670 - val_loss: 0.8814\n",
      "Epoch 16/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3654 - val_loss: 0.5692\n",
      "Epoch 17/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3605 - val_loss: 0.6389\n",
      "Epoch 18/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3580 - val_loss: 0.3667\n",
      "Epoch 19/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3544 - val_loss: 0.5900\n",
      "Epoch 20/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3533 - val_loss: 0.8051\n",
      "Epoch 21/100\n",
      "11610/11610 [==============================] - 0s 26us/sample - loss: 0.3540 - val_loss: 0.7272\n",
      "Epoch 22/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3502 - val_loss: 0.4839\n",
      "Epoch 23/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3548 - val_loss: 1.4535\n",
      "Epoch 24/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3541 - val_loss: 0.7182\n",
      "Epoch 25/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3475 - val_loss: 0.6371\n",
      "Epoch 26/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3468 - val_loss: 0.7073\n",
      "Epoch 27/100\n",
      "11610/11610 [==============================] - 0s 26us/sample - loss: 0.3450 - val_loss: 0.4386\n",
      "Epoch 28/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3435 - val_loss: 0.4745\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=my_mse)\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, validation_data = (X_valid_scaled, y_valid),\n",
    "          epochs = 100, \n",
    "          callbacks = [tf.keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3)\n",
    "Try building and compiling the model again, this time adding `\"mse\"` (or equivalently `\"mean_squared_error\"` or `keras.losses.mean_squared_error`) to the list of additional metrics, then train the model and make sure the `my_mse` is equal to the standard `mse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 1.8951 - val_loss: 0.9107\n",
      "Epoch 2/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.5979 - val_loss: 0.5217\n",
      "Epoch 3/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.4860 - val_loss: 0.4757\n",
      "Epoch 4/100\n",
      "11610/11610 [==============================] - 0s 26us/sample - loss: 0.4388 - val_loss: 0.4460\n",
      "Epoch 5/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.4133 - val_loss: 0.3778\n",
      "Epoch 6/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.4043 - val_loss: 0.4112\n",
      "Epoch 7/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3945 - val_loss: 0.3752\n",
      "Epoch 8/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3927 - val_loss: 0.5582\n",
      "Epoch 9/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3828 - val_loss: 0.7539\n",
      "Epoch 10/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3831 - val_loss: 0.4400\n",
      "Epoch 11/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3734 - val_loss: 0.8473\n",
      "Epoch 12/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3773 - val_loss: 0.6332\n",
      "Epoch 13/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3652 - val_loss: 0.4820\n",
      "Epoch 14/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3609 - val_loss: 0.7063\n",
      "Epoch 15/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3588 - val_loss: 0.8860\n",
      "Epoch 16/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3636 - val_loss: 0.6500\n",
      "Epoch 17/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3532 - val_loss: 0.4554\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "              loss=\"mse\")\n",
    "history = model.fit(X_train_scaled, y_train, validation_data = (X_valid_scaled, y_valid),\n",
    "          epochs = 100, \n",
    "          callbacks = [tf.keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4)\n",
    "If you want your code to be portable to other Python implementations of the Keras API, you should use the operations in `keras.backend` rather than TensorFlow operations directly. This package contains thin wrappers around the backend's operations (for example, `keras.backend.square()` simply calls `tf.square()`). Try reimplementing the `my_mse()` function this way and use it to train and evaluate your model again. **Tip**: people frequently define `K = keras.backend` to make their code more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "11610/11610 [==============================] - 0s 33us/sample - loss: 1.9328 - val_loss: 0.6994\n",
      "Epoch 2/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.5660 - val_loss: 0.5170\n",
      "Epoch 3/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.4645 - val_loss: 0.5235\n",
      "Epoch 4/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.4177 - val_loss: 0.7401\n",
      "Epoch 5/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.4004 - val_loss: 0.5230\n",
      "Epoch 6/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3875 - val_loss: 0.7618\n",
      "Epoch 7/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3829 - val_loss: 1.3310\n",
      "Epoch 8/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3787 - val_loss: 0.8830\n",
      "Epoch 9/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3719 - val_loss: 0.4616\n",
      "Epoch 10/100\n",
      "11610/11610 [==============================] - 0s 26us/sample - loss: 0.3688 - val_loss: 0.7551\n",
      "Epoch 11/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3694 - val_loss: 0.4207\n",
      "Epoch 12/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3584 - val_loss: 0.4935\n",
      "Epoch 13/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3570 - val_loss: 0.4586\n",
      "Epoch 14/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3520 - val_loss: 0.4013\n",
      "Epoch 15/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3488 - val_loss: 0.3749\n",
      "Epoch 16/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3499 - val_loss: 0.9949\n",
      "Epoch 17/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3506 - val_loss: 0.8523\n",
      "Epoch 18/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3455 - val_loss: 0.7120\n",
      "Epoch 19/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3434 - val_loss: 0.4298\n",
      "Epoch 20/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3374 - val_loss: 0.3319\n",
      "Epoch 21/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3345 - val_loss: 0.4029\n",
      "Epoch 22/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3322 - val_loss: 0.4061\n",
      "Epoch 23/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3298 - val_loss: 0.5609\n",
      "Epoch 24/100\n",
      "11610/11610 [==============================] - 0s 30us/sample - loss: 0.3303 - val_loss: 0.5224\n",
      "Epoch 25/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3305 - val_loss: 1.3521\n",
      "Epoch 26/100\n",
      "11610/11610 [==============================] - 0s 30us/sample - loss: 0.3334 - val_loss: 1.8828\n",
      "Epoch 27/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3342 - val_loss: 1.1121\n",
      "Epoch 28/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3280 - val_loss: 0.6393\n",
      "Epoch 29/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3226 - val_loss: 0.4807\n",
      "Epoch 30/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3198 - val_loss: 0.6711\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def my_mse_keras(y_true, y_pred):\n",
    "    return K.mean(K.square(y_true - y_pred))\n",
    "    \n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "              loss=my_mse_keras)\n",
    "history = model.fit(X_train_scaled, y_train, validation_data = (X_valid_scaled, y_valid),\n",
    "          epochs = 100, \n",
    "          callbacks = [tf.keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f7cf3d9e6a0>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD7CAYAAAB68m/qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO2dd5xcZbn4v8+Ubdky6cluNgkhhPSegEoPNSiIXoRQBK6AIliuys9yvYKKIqDo9YogaqRcRLiIEgRFkNCUloSSXkhIspuwLdmSbJ95f3+8M5vJZsvMzpmdMzPP9/PJ58ycc+Y9z2SS5zznqWKMQVEURUl/PKkWQFEURXEGVeiKoigZgip0RVGUDEEVuqIoSoagCl1RFCVDUIWuKIqSIfSr0EWkXERWisgGEVkvIl/q4RwRkZ+LyDYReVdE5idHXEVRFKU3fDGc0wl81RizRkSKgNUi8qwxZkPUOecAx4T/HAfcHd4qiqIog0S/Ct0YsxfYG37dJCIbgTIgWqGfDzxgbJXSayISEJGx4c/2yIgRI8zEiRMTEl5RFCXbWL16da0xZmRPx2Kx0LsQkYnAPOD1bofKgN1R7yvC+3pV6BMnTmTVqlXxXF5RFCXrEZGdvR2LOSgqIoXAH4EvG2MaByjItSKySkRW1dTUDGQJRVEUpRdiUugi4scq84eMMY/3cEolUB71flx432EYY+41xiw0xiwcObLHJwZFURRlgMSS5SLAb4GNxpg7ezltBfDpcLbL8UBDX/5zRVEUxXli8aF/BLgcWCsib4f3fQsYD2CMuQd4GlgKbAOagasGIkxHRwcVFRW0trYO5ONZQ15eHuPGjcPv96daFEVRXEQsWS6vANLPOQa4PlFhKioqKCoqYuLEidgHA6U7xhjq6uqoqKjgqKOOSrU4iqK4CFdVira2tjJ8+HBV5n0gIgwfPlyfYhRFOQJXKXRAlXkM6N+Roig9kTKFXt3UlqpLK4qiZCSpU+iNrbhx/F1hYWGqRVAURRkQKVPoBmjtCKXq8oqiKBlHSn3o9S3tqbx8nxhjuPHGG5k5cyazZs3ikUceAWDv3r2cdNJJzJ07l5kzZ/Lyyy8TDAa58soru8796U9/mmLpFUXJRuLq5eI0DS0djC3J7/HYd59cz4Y9A+ow0CvTS4u56WMzYjr38ccf5+233+add96htraWRYsWcdJJJ/H73/+es846i//8z/8kGAzS3NzM22+/TWVlJevWrQOgvr7eUbkVRVFiIbUWenNHKi/fJ6+88grLli3D6/UyevRoTj75ZN58800WLVrE7373O26++WbWrl1LUVERkyZNYvv27XzhC1/gb3/7G8XFxakWX1GULCSlFnpfCj1WS3qwOemkk3jppZd46qmnuPLKK/nKV77Cpz/9ad555x2eeeYZ7rnnHh599FGWL1+ealEVRemNve9CcSkMGZFqSRwlpRZ6Y4t7LfQTTzyRRx55hGAwSE1NDS+99BKLFy9m586djB49mmuuuYarr76aNWvWUFtbSygU4pOf/CS33HILa9asSbX4iqL0RigI950LL96eakkcJ7UWuouDohdccAGvvvoqc+bMQUS4/fbbGTNmDPfffz933HEHfr+fwsJCHnjgASorK7nqqqsIhWzWzq233ppi6RVF6ZX970NbI+x7L9WSOE7Kg6Ju48CBA4Ctxrzjjju44447Djt+xRVXcMUVVxzxObXKFSVNqFpvt/W7+z4vDUmZy8XrEVcHRRVFyVC6FPoucGFxYyKkTKH7PEK9Cy10RVEynOqwQu9sgea61MriMCm10N0cFFUUJUOpWg/+IfZ1/a7UyuIwqVPooi4XRVEGmfaDsG8HHH2qfd+QWX70WEbQLReRahFZ18vxEhF5UkTeEZH1IhLTtCKvR1yd5aIoSgZSvQkwMOVs+z4LLfT7gLP7OH49sMEYMwc4BfiJiOT0t6jXIzSoha4oymAS8Z9P+DDkFGVcpku/Ct0Y8xKwr69TgKLwMOnC8Lmd/a3r9QiNrZ0EQ+kbZe6r1e7777/PzJkzB1EaRVH6pWoD+Atg6FEQKM8+l0sM/AKYBuwB1gJfMsb02xfX67FTdzQwqijKoFG1DkZNA48HAuOz0uXSH2cBbwOlwFzgFyLSY3cqEblWRFaJyKrmg7aAx03FRd/4xje46667ut7ffPPN3HLLLSxZsoT58+cza9YsnnjiibjXbW1t5aqrrmLWrFnMmzePlStXArB+/XoWL17M3LlzmT17Nlu3buXgwYOce+65zJkzh5kzZ3a17VUUJUGMsRkuo8N9okrKM87l4kSl6FXAj4wdP7RNRHYAU4E3up9ojLkXuBfg2JlzTBv0nov+12/AB2sdEC+KMbPgnB/1eviiiy7iy1/+Mtdffz0Ajz76KM888wxf/OIXKS4upra2luOPP57zzjsvrrmed911FyLC2rVr2bRpE2eeeSZbtmzhnnvu4Utf+hKXXnop7e3tBINBnn76aUpLS3nqqacAaGhoSOw7K4piOVAFLftgVFihB8qhrQFaGyCvJLWyOYQTFvouYAmAiIwGjgW29/ehiMulvtk9mS7z5s2jurqaPXv28M477zB06FDGjBnDt771LWbPns3pp59OZWUlVVVVca37yiuvcNlllwEwdepUJkyYwJYtW/jQhz7ED3/4Q2677TZ27txJfn4+s2bN4tlnn+XrX/86L7/8MiUlmfEPTVFSTqRCNNpCh4yy0vu10EXkYWz2yggRqQBuAvwAxph7gO8D94nIWkCArxtjavtb1+ux95JeXS59WNLJ5MILL+Sxxx7jgw8+4KKLLuKhhx6ipqaG1atX4/f7mThxIq2trY5c65JLLuG4447jqaeeYunSpfzqV7/itNNOY82aNTz99NN8+9vfZsmSJXznO99x5HqKktV0V+iBCXZbvwvGZEYCQ78K3RizrJ/je4Az471wxEJ3kw8drNvlmmuuoba2lhdffJFHH32UUaNG4ff7WblyJTt37ox7zRNPPJGHHnqI0047jS1btrBr1y6OPfZYtm/fzqRJk/jiF7/Irl27ePfdd5k6dSrDhg3jsssuIxAI8Jvf/CYJ31JRspDqDVA4BgqG2feBsIWeQZkuKeu26A37oN2Wiz5jxgyampooKytj7NixXHrppXzsYx9j1qxZLFy4kKlTp8a95uc//3muu+46Zs2ahc/n47777iM3N5dHH32UBx98EL/f3+XaefPNN7nxxhvxeDz4/X7uvvvuJHxLRclCogOiAENGgi8vozJdxKSo29jChQtN89JbWLZ4PP/10ekAbNy4kWnTpqVEnnRD/64UJQ6CnfDDUjjuWjjzlkP7/2ehTWO86MHUyRYnIrLaGLOwp2MpnVgUyPdrPxdFUZLPvvcg2Aaju/nKM6y4KKUDLkoKclznQ4+XtWvXcvnllx+2Lzc3l9dffz1FEimKcgRV4VZUo6Yfvr+k3M4XzRBSq9DzfTSkeYOuWbNm8fbbb6daDEVR+qJqA4gXRh57+P7AeGiuhfZmyClIjWwOkmKXS84RLpdU+fTTCf07UpQ4qVoPI44BX+7h+wPj7TZD3C6pVegF/sNcLnl5edTV1anC6gNjDHV1deTl5aVaFEVJH6rXH+lugYwrLkqxy8VPfUsHxhhEhHHjxlFRUUFNTU0qxXI9eXl5jBs3LtViKEp60NpoUxPnHznc/VAuemakLqY4KOqnvTNEa0eI/Bwvfr+fo446KpUiKYqSaVRvtNvoHPQIRWPB48uYXPSU+9DBfdWiiqJkENXdSv6j8XihuCxjXC4pVegl+X4AHUWnKEryqFoPucWH/OXdCYzXoKgTBArCCl2LixRFSRZVG2xAtLeW14HxaqE7QcRCV5eLoihJoWuoRQ8ZLhFKyqFpL3Smv6fAHQpdLXRFUZJBY6UdYtFTymKEQDlgoLFi0MRKFq5wuaiFrihKUqjaYLfde7hEk0G56P0qdBFZLiLVIrKuj3NOEZG3RWS9iLwY68ULc314PaJBUUVRkkNXD5c+OpNGqkUzIHUxFgv9PuDs3g6KSAD4JXCeMWYGcGGsFxcRW1ykLhdFUZJB9QZrgecHej+nuAyQjMh06VehG2NeAvb1ccolwOPGmF3h86vjESCQ71eXi6IoyaGql5L/aHw5tsAoG1wuMTAFGCoiL4jIahH5dDwfLlaFrihKMuhsh9otPRcUdSdDctGdKP33AQuAJUA+8KqIvGaM2dL9RBG5FrgWYPx467cKFPipO6A+dEVRHKZ2C4Q6Y1To5bA7/WcYOGGhVwDPGGMOGmNqgZeAOT2daIy51xiz0BizcOTIkYC6XBRFSRLVkQyXGBR6STk07oFQMLkyJRknFPoTwAki4hORAuA4YGOsH7ZBUbXQFUVxmKp14PHD8Mn9nxsot9Z8097ky5VE+nW5iMjDwCnACBGpAG4C/ADGmHuMMRtF5G/Au0AI+I0xptcUx+6UFOTQ2NpJMGTwenopzVUURYmXqg12QpHX3/+5XamLu6EkfVtT96vQjTHLYjjnDuCOgQgQCFeLNrV2ECjIGcgSiqIoR1K9ASaeENu5JVG56BM+lDyZkkxKK0UhquOi5qIriuIULftt2X9/KYsRIlZ5mg+6SLlC7+q4qIFRRVGcIpaS/2hyCmDIyLTPRU+5QteOi4qiOE5VZKhFjBY62EyXNC//T7lCP9QTXTNdFEVxiOr1kD/UVoDGSqA87YuLUq7QS8Jj6BrVQlcUxSmq1sOoGb0PteiJwHhoqLA91NMUFyh0DYoqiuIgoZAdDB2PuwVspktnKxysSY5cg0DKFXqOz0NBjleDooqiOEPDLmg/EFuFaDSBSF/09PWjp1yhg5b/K4riIJGA6Kg4FXqJKnRHKNae6IqiOEUkZbGvoRY9EbHQ0zgw6gqFHijw06BTixRFcYKqdTB0IuQWxve5vBL7Ry30xAjk56jLRVEUZ6jeEHtBUXdKxqd1cZErFLqOoVMUxRE6WqBuW+wl/91J80EXrlDogQK/ZrkoipI4NZvBhOJPWYwQKLcWeprmortCoZcU+GnvDNHakd7N5RVFSTHVcfZw6U5JObQ32eZeaYg7FLoWFymK4gRV68GXB8MmDezzaZ7p4gqFHgiX/9drpouiKIlQtR5GTgWPd2Cfjx50kYb0q9BFZLmIVItIn1OIRGSRiHSKyL/FK0RXx0W10BVFSYSq9fFXiEYTPegiDYnFQr8POLuvE0TEC9wG/H0gQmhPdEVREuZADRysTkyhFwwDf0HmulyMMS8B+/o57QvAH4HqgQihPdEVRUmY6kjJ/wAzXMB2ZwyMz2gLvU9EpAy4ALh7oGuUFKjLRVGUBIl3SlFvlKRvX3QngqI/A75ujAn1d6KIXCsiq0RkVU3NoRaVRbk+vB7RoKiiKAOner0dI1c4MrF1Auk7ucjnwBoLgT+IbSQ/AlgqIp3GmD93P9EYcy9wL8DChQu7MvdFhBLtuKgoSiJUrU/M3RIhMN7mobcdiL8fTIpJ2EI3xhxljJlojJkIPAZ8vidl3h9a/q8oyoAJBaF6U+LuFjjURjcN3S79Wugi8jBwCjBCRCqAmwA/gDHmHqcEUQtdUZQBs28HdLYMvOQ/mkBU6mK8LXhTTL8K3RizLNbFjDFXDlSQQIGffQfVh64oygCIZLgkkrIYIY0HXbiiUhTU5aIoSgLUbLbbEccmvlbhaPDmpKXLxTUKPZDvp75ZLXRFUQZAw26b4ZJTkPhaHg+UjEvL8n/XKPSSghya2joJhtKzbWXCHKiBx6+FlvpUS6Io6UdDJRSXObdeSXqmLrpHoef7MQaaWrPU7bL17/DuI/DeP1ItiaKkH42V1qp2ijQddOEahR7I9ha6NRvttnJNauVQlHSkIQkK/UAVdLQ6t+Yg4BqFnvX9XCJBncrVqZVDUdKN1gY7lMJplwtAQ4Vzaw4CrlHoWd9xsWaT3e59B4KdqZVFUdKJhkq7LXFQoXcNukgvP7rrFHpWWuhtB2wAZsSx0NF8SLkritI/jWGFXuywywXSLtPFNQq9uGvIRRamLtZusdu5l9itul0UJXYibhEnLfSiUhBv2gVGXaPQs3quaMR/fuxSyAuoQleUeGisBPFA4Rjn1vT6oLg07VIXXaPQc31eCnK82elyqdkIHr8dbFs2XzNdFCUeGiqhaKxVwk4SGK8ul0QoyfdnZ1C0ZjOMOMb+gyxbANUboL051VIpSnrQWOFshkuENBx04T6Fno0ul+qNdlI5WIVugvDBu6mVSVHShYYKZ/3nEQLl1p0TTB+d5CqFHijw05htFnr7Qeuniyj00vl2q350RekfY6BxT3Is9MB4MCG7fprgKoVuXS5ZluVSuxUwMDLcJa5otH3UU4WuKP3TXAedrc5WiUZIw0EX/Sp0EVkuItUisq6X45eKyLsislZE/iUicwYqTCA/J/tcLpGc8+hG+mXzVaErSixEUhaTZaFDWgVGY7HQ7wPO7uP4DuBkY8ws4PuEZ4YOhEBBFk4tqtl0KMMlQul82P8+HKxLmViKkhY0JqFKNELkJpFGqYv9KnRjzEvAvj6O/8sYsz/89jVgwM8+xfl+2jpDtHYEB7pE+lG9CYZPBq//0L6yBXa7563UyKQo6UJX2X+582v782xuexqV/zvtQ/8M8NeBfrirn0s2uV1qNh3yn0conQuIul0UpT8aK+x0oYIRyVk/UJ5xLpeYEJFTsQr9632cc62IrBKRVTU1NUccz7qOix0t1rUSyXCJkFtk96lCV5S+aai0FZ2eJOV3pNmgC0f+FkRkNvAb4HxjTK+OX2PMvcaYhcaYhSNHjjzieCA/ByB7RtHVbgEMjJp65LGyBVahmyyd4KQosdBY6WxTru4ExttrhELJu4aDJKzQRWQ88DhwuTFmSyJrZV0L3UgPl+4WOkDZPGiuTSvrQFEGnYbK5AREIwTKIdhuh12kAf02PxCRh4FTgBEiUgHcBPgBjDH3AN8BhgO/FBGATmPMwoEIk3Uul5pN4PHBsKOPPBYJjFauhqETBlcuRUkHQkFoSlJRUYSScOpiw24oHpu86zhEvwrdGLOsn+NXA1c7IUxJpCd6tgRFqzdZZe7LOfLYqBngzYU9a2DmJwZfNkVxOweqIdSZfAsd7JNy+eLkXcchXFUpWpTrw+uR7LLQu2e4RPDlwNjZ2nlRUXojGYMtulMSpdDTAFcpdBGhOM+XHeX/Ha2wf8fhFaLdKVtgc9F1JJ2iHEkyBlt0J7cQ8oelTfm/qxQ6QKAgS8r/67baxj+9WehgFXpHM9RuHjy5FCVdSGbZfzSB9ElddJ1CL8nPkvL/vjJcImjnRUXpncZK8BdA/tDkXieNBl2oQk8V1RvtzMLhk3s/Z9gkyCtRP7qi9ERDeLCFza5LHiXjrcslDWpCXKfQAwVZMuSiZpNV2L7c3s/xeKyVrha6ohxJY5Jz0CMEyq3rs9n9zfLcp9CzxUKv2dxzhWh3yhZA1XrbJkBRlEM0JLlKNEIkzlXVYwdxV+E6hV6S76extYNgyP2PNwOmsw32be/bfx4hMpJur46kU5QuOsPVm8kYbNGdsXPtds/byb9WgrhPoRfkYAw0tWawlV63zSrpmBS6BkYV5Qia9gJmcFwuBcNsYHSvKvS4yYry/+qNdhuLQi8aYwM/qtAV5RBdRUWDoNDBWulqocdPID8LeqLXbAbx9J3hEk3ZfNsCQFEUS9dgi0FwuYCdUbB/B7Ts7//cFOI+hZ4NHRdrNsHQo+xElFgoW2B97s29Do5SlOyicZCKiiJE/Oh73xmc6w0Q1yn0rHC51Gzqu+S/O10j6dRKVxTAWuh5JbY0fzAonWe3Lne7uE+hd3VczNB+Lp3tUPde3yX/3RkbGUmnCl1RgOQPtuhOwTBbYOTywKj7FHqmW+hdGS5xWOh5xTBiigZGFSVCQ8XgZLhEUzpHLfR4yfV5yfd7MzcoWrPJbuOx0EFH0ilKNI2Vg+c/jzA2EhitH9zrxkG/Cl1ElotItYj0WCYllp+LyDYReVdE5icqVKDAn7lB0UiGy4hj4vtc2Xw4WHOow5yiZCvt4TL8QbfQ3R8YjcVCvw84u4/j5wDHhP9cC9ydqFAZ3aCrZiMMnQj+/Pg+Fz2STlGymcY9djuYPnSAseHAqIv96P0qdGPMS0Bf+XLnAw8Yy2tAQEQSGr5Xku/P3DF0NZtjKyjqzuiZ4M1Rha4ojYMw2KInhgy3E4xc7Ed3wodeBkQ3C64I7xsw1uWSgVkuwQ4bFI3Xfw52JN0YHUmXMg7UwL/+x/6GSmppGOQq0WjGzklvC91JRORaEVklIqtqamp6PS+Qn5OZLpe69+xQ23gyXKIpm29H0oWCzsql9M+6x+Dv34ZXfpZqSZTBLvuPpnSuLfJrbRj8a8eAEwq9EiiPej8uvO8IjDH3GmMWGmMWjhw5stcFSzK1J/pAM1wilC2AjoNQu8U5mZTYiPydv3gbVG1IrSzZTkMFDBkZe6W1k3T50d0ZGHVCoa8APh3OdjkeaDDG7E1kwZJ8P22dIVo7MswSrdkEiM0pHwgaGE0dtVvt75ZXAk98Xgd3p5JUpCxGKHV3K91Y0hYfBl4FjhWRChH5jIh8TkQ+Fz7laWA7sA34NfD5RIXK2OKimk0wdALkFAzs88OOhtwSVeipoO49KFsIS++wbq9X/yfVEmUvDZWD15SrO0NG2Owal/rRff2dYIxZ1s9xA1zvmERENehq7mB0cQoeq5LFQDNcIng8UDZPFfpg03YAmvbAiMkw4wJY/zisvBWOXTpw95kycBor4aiTUnf9Uve20nVdpSjYoChAfSb1cwl22Mf2RBQ66Ei6VFC3zW6HH2MHEp97p33KeuJ6DVAPNq2N0NY4+CmL0YydC/vec2Vg1JUKPSNdLvt2QKgjcYVeOt9mynyw1hm5lP6JKPRIdW/hKDjndqh4E177ZerkykZSmeESIdJ50YVjIV2p0DOyJ3pNZEpRgo/oXYFRzUcfNGq3AmJ72EeYdSFMOQeevwVqt6VMtKxjsAdb9ERXCwD3uV1cqdAjLXQbM0qhb7bbRBV68VgoKlU/+mBSt9XOlIxOkxOBj/4UfLmw4gYIhVInXzYx2IMteiISGHWhH92VCr0wx4dHMmwMXc0mqxRyhiS+Vtl8VeiDSe3WnpupFY+Fs38Eu16FN+4dfLmykYZK29yuKKHuIolTOlct9FjxeISS/Awr/6/eNPAK0e6ULbBBGR1Jl3yMsSmLw3vpjjlnGUw+A/7xXVtBqCSXhgooHAPefhP0ksvYuTa20tqYWjm64UqFDhAoyKGhJUOKN4Kd9rHdqRS3snCH4j1vObOe0juNe2x17oheBnqLwMf+Gzw+WPFFdb0km8YUDLboiYgf/QN3BUZdq9CL8/2Zk7a4/30Itiee4RIhEmXXwGjyqdtqt71Z6GAVzJm3wPsvw+rlgyNXttKQwirRaMa6s2LUtQo9kEk90SMZLqMcUuh5JTqSbrDonrLYG/M/DZNOhWdvgvpdyZcrGzHGpi2mMsMlQuFIe2NxmR/dvQq9IJMUergp1wgHqwrLFto8aB1Jl1xqt0FOYf9BOBE47+f29Yov6u+SDJr3QWerOyx0sFa6WuixUZKfQR0XqzfZieG5hc6tWb4ImmvtjEMledRtheFHW4XdH4HxcMZ3YftKWPNA8mXLNroGW7jAQgfrR6/bBm1NqZakC9cq9EC+n8bWDkKhDLB0ajY73/Oj/Di73f2ms+sqh1O7FYb3EhDtiQX/DhNPtL3Tdf6rs3QVFbnIQse4qmLUtQq9ON+PMdDUmuaZLqGg7aXttEIfORVyimD3686uqxyio9X6w/sKiHbH44Hz/se2Z3jyS+p6cZKusn8XWejgKj+6axV6oCDcoCvdc9H3vw/BNhjlUA56BI8Xxi2AijecXVc5xL7tgOk/INqdYUfB6d+Fbc/BWw8mRbSspKECPH473MINFI6yVdsu8qO7V6HnH2qhm9Z0TSlyKMMlmnGLbefFtgPOr61EpSzG4XKJsOhq63r527egfnf/5yv901gJxaX2KcgtuKxiNKa/GRE5W0Q2i8g2EflGD8fHi8hKEXlLRN4VkaWJChbp55L2mS5dGS4DnFLUF+XHgQnBHs1HTwq1CSh0jwfO/4X9fVZ8QV0vTpDKwRa9MXau/XfiksBoLBOLvMBdwDnAdGCZiEzvdtq3gUeNMfOAi4GEe4p2WejprtCrN1mfX16x82uPC3deVD96cqjbZh+pB5qdNHQinPl9m/Wy+j4nJctOGivck7IYoTQcGHVJO+tYLPTFwDZjzHZjTDvwB+D8bucYIKKxSoA9iQqWURZ6sqba5A+1ue2a6ZIc6rb1XvIfKwv/HY462Wa97N/pjFzZSCgEjXvdk+ESwWUVo7Eo9DIg2glYEd4Xzc3AZSJSgZ0x+oVEBesacpHO5f+RDBenA6LRlC/SAqNkYEw4ZTHOgGh3RKzrBbTNbiIcrLYDYtxmoReNtkVnLvGjOxVdWAbcZ4wZBywFHhSRI9YWkWtFZJWIrKqpqelzwVyfl3y/N72DovU7bWVbMudOjlsMLftsR0DFOZrroLV+YP7z7gTGw1k/gB0vwarfJr5eNtLgsqKiaFxUMRqLQq8EyqPejwvvi+YzwKMAxphXgTxgRPeFjDH3GmMWGmMWjhzZf+pR2pf/dw21SEKGS4SuAiP1oztKJCAab8pib8y/Ao4+DZ79jh1HqMRHgwsGW/RG6Vz7JO6CbLNYFPqbwDEicpSI5GCDniu6nbMLWAIgItOwCr1vEzwGbE/0NFbo1Q6NneuLEVNssy7NR3eWRFIWe0LEFhx5fOHh0up6iYtGF4ye642x7gmM9qvQjTGdwA3AM8BGbDbLehH5noicFz7tq8A1IvIO8DBwpTGJO3VL8v00pLPLpWazzZLIK0neNTwe26hLA6POUrsVvLnWXeIUJePg7Fth5z91wlG8NFSCL98mArgNF1WMxjT2wxjzNDbYGb3vO1GvNwAfcVY063J5v7bZ6WUHjw/WJtc6j1C+GF74kZ2e4kR6ZEerDUDlFiW+VrpStw2GTbIVuU4y91LY8AQ8dzMcc4Zt/KX0T2SwRSxN0gabojF2ipIL/OguKrk6krQeQ/fBWqheD1POSv61yhcDBmeOpHwAACAASURBVCpXObPeX/4DfnumM2ulK7VbE09Z7InIhCNvDvz58zYTSukfNxYVReOSilFXK/RAQU7ys1yqNyZnNufq++0j++yLnF+7O2ULAXHG7dLRai3I6g3ZmzkT7LBtiRNNWeyN4lI45zbY/Rq8fk9yrpFpNFa6pylXT4wNB0bbD6ZUDFcr9JJ8P22dIVo7kmTFtOyHXy+BJ25wdt32Znj3UZh+HhQMc3btnsgrtrnuTgRGt6+0MzQBtv0j8fXSkf07bbdEpzJcemLOxTDlHPjH9w5l1Cg9E+yApg/cV1QUTelc2+YhxYFR1yt0SGK16JoHrPLa/LSzE9s3PAFtDTZVbbAYFy4wSjR7YsMKG8QdOtF2C8xGImPnkmWhQ9j18jPw5cGfr1PXS1807QWMO1MWI7ikYtTVCj1QkMSOi8FOeOPXMGaWDXy98Wvn1l5zPww7Giae4Nya/VF+HLQ2HEq3GwjBDntzm3IOHHOWHXrc0eqcjOlCV8pikgOWRWNg6R32RvzqL5J7rXTGbYMteqJ4LBSOTrkf3dUKPakW+uanoGE3nPJNmH4+vPW/znRMq9kMu161Q4MHMyJfvthuEykwev8VWx057WMw+XToaIZd/3JGvnSidisUDB8cd9msC2HqR+H5H+hw6d5w22CL3nBBxWjqFHrL/n5PCeSHh1wko5/La3dDYAJMORuOuw7aGuHt3ye+7poHbPHI3EsSXysehk+2Obq7E/Cjb3wS/AUweQlM/IgN6majH71uW3LdLdGIwFk/tENQ1v1xcK6ZbnSV/bvYQodwxejmlAZGU6fQ63f2a5EEktVxcc9b1oo+7rPW3VK+yGaKvP6rxHzQnW3wzsNw7FI7zWQwETnkRx8IoRBs+ovNjfbnQ84QmPDh7PSjJytlsTeGToCyBbD+T4N3zXSisRJyS9xfFzE2EhhdlzIRUuhyEXjx9j7PKE6Wy+W1eyCnEOZddmjf8dfBvvdg27MDX3fTU7ap04JBDIZGU77YtuttqY//sxVvwIEqmHbeoX2TT7frZdPEndYG29lvsCz0CDMugL3vOBuczxQaKt1vnYMrKkZTp9ALRlgXRx+5zkW5PjzicFC0qco+2s699PCS/Onn2zaYr9098LXX3A8l5TDp1MTlHAjjwn70igEUGG180ha7HBNVUDT5dLt9L4vcLrXhDJdkpiz2xPTwiIH1fx7c66YDDbvdneESoWgsDBmVXD96Z1ufh1On0ItGgS8XXri111M8HrH9XJy00FcttznGx3328P1ePyz6jM3Drt4U/7r7dsD2F2De5c6Xi8dK2QIQT/z56MbAxhUw6ZTDWweMPNbeoLLJ7dKV4TLICj0w3rr9NqhCP4LGNLHQRZJfMfqnz/Z5OHUK3eOHxdfC2segakOvpznacbGzzfajnnJWzylpC66ygcCBVO+99aBVptFunMEmtxBGzYg/MPrBuzaeMe1jh+8XsQHS7S/alMZsoHYriNfm4Q82Mz6ubpfudLRYN6bbM1wijJ1r3ZTtSehBVb2p3ye41KYtfuRLNtDxwg97PaWkIMe5LJd1f4SDNXDc53o+PmQEzL4Q3vlDfO0Agp3w1kMw+YzUWxLli6BydXyFKhuftDejY8898tjk020G0ECDrelG3TarzH05g39tdbscSWN4mmWq/1/FSqRitCoJgdGX7rBZaH2QWoVeMAw+dL1VKHve6vGUQL6fRicsdGOsf3zkNOta6I3jroPOFusPj5Wtz8CBD1IXDI2m/DirgGvicBttWAETPgJDhh957KiTbBrm1gSCxelE3TbneqDHi7pdjsTNgy16IlkVozVbrEG6+Oo+T0t9YdHx19n86ed/0ONhx1wuO/9lXQvHf67vgp8xM2HiifDGb6zlHQur77ftM48ZhM6K/TFukd3G6nap2WxzZ6OzW6LJK7E3iWzwo4dCNkg/2AHRaDTb5XDcPNiiJ4pLYchIW2XtJC//2KYTf/iLfZ6WeoWeV2JdL9uehV2vHXE4UOB3Jsvl9bvtjWPWp/o/9/jrbP/lTU/2f25DpZV93qXgjam9fHIZNslWOcbqItkY/o5Te3C3RJi8xN4Mm6oSl8/NNFbYp7NUWeigbpfuRMr+08VCF7FxtI0rEivyi6Z2G6z9P5u0MeSIyZ6HEZNCF5GzRWSziGwTkW/0cs6nRGSDiKwXkfhKLhdfa9N9nr/liEMl+X4aWzsIhRIYgLR/p80RX3AV5PTtgwJs9Whggs1X74+3/tf6zOZdPnD5nETEpi/G+o9p45P2Mb8vH2VX+uLzicvnZpyeIzoQAuX2KUuLjCyNFTbF2Z+Xakli58Sv2UllT3/NmaZrL//YJmv0Y51DDApdRLzAXcA5wHRgmYhM73bOMcA3gY8YY2YAX45L4JwhcOJX7WPK9hcPO1SS78cYaGqN0f3RE2/cCwgs6tv/1IXHa9Mad7/Wq28fsD/WWw9an/ywowYun9OUL7bpd/0Fdut32RSr6b24WyKMnmVvuJnudhmMLouxMP3j9okoW/vRR5MuRUXR5BbCWbdY11k8sbieqHvPtuJe+O8xVZ/HYqEvBrYZY7YbY9qBPwDndzvnGuAuY8x+AGNMdZxiw4Ir7WPV87fYAGaYQEG4n8tAJxe1HYA1D9pH2Xj+Ycy7zFaT9mWlv7fSFj0MZpvcWIg06urP7bLxL3Y79aN9n+fxWLfLe89ndpvX2q2QWzz4bRu6E3G7aHDU/YMtemPGJ2ws7h/fS2yAzst32hqZj/RvnUNsCr0MiK79rgjvi2YKMEVE/ikir4nI2TFdPRp/Hpx0oy2KicqoSLjj4jsP297kx38+vs/lldgGW+v+2LvveM191l/dl/85FZTOs7nU/bldNq6A0TNjaxM7+XRo2ZfybnJJpW6r9Z+nem5ll9tFFXpaWuhg/w2dc7ud8/uP7w1sjX07rP5acKVttRwDTgVFfcAxwCnAMuDXIhLofpKIXCsiq0RkVU1NzZGrzLvM5gA///2uJlkJ9UQPhWyRUNkCm58dL4s/a4clr1p+5LED1bD5rzBnma14dRM5Q2y2Tl8Vo01VNgjdvZioN44+DZDMdrvUbkut/zyaGReo26WtyRpj6RIQ7c7o6dZ1u/q+vl23vfHKnTZl+COxe7BjUeiVQHnU+3HhfdFUACuMMR3GmB3AFqyCPwxjzL3GmIXGmIUjR4488kpeP5z8DfsPOZxhEkjEQn/vH9Yvetx18X8WbMe9Y8601aXdeyi8/ZBtIeA2d0uE8uOgck3vqZebnwJM7Aq9YJi9MSbSvMzNtB+0AbhU+88jqNslarBFGrpcIpzyDZuZ8vSN8XVy3b/T9rpacIUdnhEjsSj0N4FjROQoEckBLgZWdDvnz1jrHBEZgXXBDCyRdvanYMQUWPlDCAW7XC4DykV/7W7bMGd6d5d/HBx/na0uXff4oX3G2L7n4z8MI6cMfO1kMm4xtB+ww557YuOTNsVx1PSej/fE5NNtFWoyhmqnmkjed7KnFMVKyTjNdmlMs6KinsgrgTO+Z+NZ7zwc++deudNWb8dhnUMMCt0Y0wncADwDbAQeNcasF5HviUgkPeIZoE5ENgArgRuNMXVxSdIlkRdO/ZatdFz3R0rCLpd3d8fZErZms7XQF30msTLuSafCyKnw2i8PBWvff9kqADdUhvZGxMXUk9ulZT/seMkWE8XjL558uk3R3L7SGRndhBtSFrsz4wI7dDhT3C4Hqu3AlFgL9tJlsEV/zL7YGljP3RRba+v63baVyLzL4/7uMfnQjTFPG2OmGGOONsb8ILzvO8aYFeHXxhjzFWPMdGPMLGPMH+KSojvTzrepci/cSq6EuPLDE/m/1RXc/UIc/7Bfv8fmbi64KiFRELF+sA/etUMxwFaG5pUkZvknm8AEm2q4u4dMl81/s+6i3qpDe6NsPuQFMnOKUSRlcZhLLHTILLdLKASPXAb/+wn42Sx44UeH+rT0RkMlIPYpO53xeOzs2IO19nv3xys/tdsT/iP+S8X9icHA44HT/tNawW//nu98dDofm1PKbX/bxMNvxDB3sXmfbbA1+1P9VlbFxOyLrSJ77W679sYVMPsiW4rrVkRs+mJPFvrGJ+1jbOm8+Nb0eG1wdNtzh6WWZgS1W22r4FgKzwaLknHWsssEt8s7v7fzbo//PIyaZttm/3Qm/OHScDpsD/7lxkqb3eH1D768TlM6FxZeZWtiqtb3fl5Dpa1tmXepzXaKE3cqdLDVmmUL4MXb8YTa+cmFczjl2JF8609reerdvX1/ds0DdsDx8QMMhnYnp8C6Vzb9BV76MQTb3RsMjaZ8sb0pHojKKGo7YF1RUz9qb5zxMvl0O9koGd3kUkkkZdFtzPh4+rtdmvfB3/8Lyo+HM38Alz8OX3wLPnyDfep98AL4xQL4588Pj880VKS3/7w7p/2XnTfw9P/r3SD658+sW/OErwzoEu5V6CJw2rdtYGT1/eT4PNx96QIWjB/Klx95i5e29JD2CNY/98avbVL/6BnOybPoGkDgtbvsjWbMTOfWThbjeigw2vYcdLbGnt3SnclLDq2TKRjjrpTFaLp6u6Sxlf6P79rRfuf+5JARMWySDRZ+ZSN84jdQOBqe/S/4yVR4/FrY9Xr6DLaIlYJhsOQ7sPOVngeCN+617ty5l9g5swPABd2k+mDSqTDhBNvLYMYF5He28LtzcvnBY9v544NvMOHEUUwoaLNBvpZ6u23cY28CS+9wVpZAOUz7KGx4Ij2sc7CPeR6fdbtMXWr3bXzS9saY8OGBrVk0xsY3tj43IB+fKzlQBe1N7klZjCbidtnwZzjpa6mWJn4qVlkl9aHrezaCfLl2BsHsC+2gm1XLrbv03Ufs8Snx1yi6mvlX2Lz0v3/bDtqJHnz9z5/Z2NYArXNwu0IXsb70350DP7aPw0XAjwC8wL/C53lzbCfF/KHW1z3/CvuX5TSnfNMqyFn/5vzaycCfD2NmHwqMdrbBlmdg5gWJjcmbvARe/YWtgoseWZeudGW4uNDlAjbb5ZlvWreLW9IqYyEUhL/8hzUCTumxp9/hjJ4O5/4YTr8Z1j1mn0oyTaF7vLD0J/Db0+3AijPCVaRNH1hFP2dZQn2h3K3QwVqS5//SjqHKD3Qp7g/a87jm/96j3hTy0OdOYfyIIcmXZdQ0+LceqkbdTPlxtkFQsMPOPG1vij+7pTuTT7fWxI6X7FNLutM1R9SlCn36+Vahr/9Telnpb/7WZof92+8Ot0T7I7fQlrsvuDJZkqWW8kUw9zJ49Zd2O3KKjR8EO+Ckrya0tHt96NHMu9Q2p5n/aev7nXgCY6Ys5CdXL6Up5Oey5W9Q3diaaindSfkiGyCuWmezc3KL7RSihNY8DnKKBuZHN6bfyeWDTt174Mt3bxOokjL7d55OvV2aqmwLj0mn2icM5XBOv8mOk/vr/7P5+auW26y8YZMSWjY9FHovTBldxO+uXETtgTYu/+0bNDgxCCPTiARGd74Km562rqhEe8/4cmDSyTYfPZ70xcY9sPxs+GEpPHyJ9ed3OjQvNhFqt1pXxkCyfgaL6R+HqrU2eJsO/P3bNvi+9Mepb3bmRgpH2QLK7Svh4Ysh2GabEyaIi/8Fx8a88UO59/KF7Kg9yFX3vUFzewJ90zORknG2MOP1u223xIFmt3Rn8hJo2HXI/9wf21+EX51kU/DmXgqVq2yhyU+OtX0uKtekLrfdrSmL0XQVGaVBtsuOl2Dto3YSmVvjEm5g0dW29Ublaph1oSPxkbRX6AAnHDOCny+by9u76/nsg6tp74yjCU6mEykwqt9l3QqR6UOJcnSM6YuhkO3p/ODHbfzj2pVw3s/hPzbApY/Z4SCr74dfnwq/PB5e+ZlN3xosOtttIyQ3pixG0+V2eSLVkvRNZzs89TVbqXxiYv7gjMfrg4/9tx0sffLXHVkyIxQ6wNkzx3LrJ2bx8tZavvDwGmoPuMxPm0oibpfJS2xrXScYOsE2UetLobfUwyOX2jzk6R+Ha56HkcfaY14fHHMGXPg7+NoW+OjPbDuF526Cn06HBz8Bax+DjhZn5O2N/TvABN2ZstidGRe43+3y2l126Pg5t7u7ktotlC+Gz77oWPZSxih0gIsWjefb507j2Q1VnHDb89zylw1UN2mwlIkfsVung1OTT4f3X4H25iOPfbAW7j0Ftv4dzr7NZgf1lumQH7Bl0Z/5O3xhjbXsarfAHz8DPw533kyWYnd7ymI0kewkt7pd6nfBi7fbKuRjMyzdME3IKIUOcPWJk3juKyezdNZYlv9zByfetpLvPbkhu7NgSufB516BmZ90dt3JS2wwZ+c/D9//9u/hN6fboNiVT8Pxn4s9MDb8aFsh/KV34dMr4OhT4cXbrDtmaxJ6sXelLKaBhe52t8vfvmm3Z9+aWjmymIxT6ACTRhZy56fm8o+vnsJHZ5dy/6vvc+LtK/nuk+upylbFPmaW89kGEz4CvrxDbpeOVnjyS/Dn62wv78++DOOPG9jaHo/NpPnUA1axe/zw0L/ZQGqkraoT1G6zZefpUiDlVrfLlmdsr6OTboTA+FRLk7VkpEKPcNSIIfzkU3N4/qsnc96cUh54dScn3r6Sm1es54OGLFXsTuLPh4knWIW+fycsP8tWu53wFbj8z1DYw1SqgTDpZLjuX7YPxtbn4BeLbPDUiZTHdMhwicaNbpeOFpupNGIKfOiGVEuT1cSk0EXkbBHZLCLbRKTXGl4R+aSIGBFZ6JyIiTNh+BDuuHAOK796ChfMLeN/X9vJSXes5DtPrGNvQ5KDbpnO5NNtL/F7TrRDbS9+2BZNeB0uQvblWN/69a/bYpXnboJfnWh9+IlQm2YKvaTMPhn96xfwgUs6Xr58J9TvtM23EhkmoySMmH5yf0XEi50RegZ2duibwDJjzIZu5xUBTwE5wA3GmFV9rbtw4UKzalWfpySN3fuauWvlNh5bXYFHhOOPHs60sUVMH1vMtLHFTBoxBJ83ox9enKPuPfjFQhg1Ay56IOFKt5jZ/Df46402EDf7Ijjj+1A0Or41mvfB7UfBmbfAh7+QHDmTwb4d8LuldoD5lU+ndgxi7Ta4+0M2i+mTv06dHFmEiKw2xvRoNMei0D8E3GyMOSv8/psAxphbu533M+BZ4Ebga25W6BF272vmt6/s4PUd+9hW3URH0P5d5Pg8TBldyLQxVsFPG1vM9LHFXePwlG5Ub7JpjIOdptbebGcv/vO/bY79ad+2IwdjbTy2+w347Rmw7JH0y8qo3WqVuscLVz3t7I20s80GtDtaobPl0LazzbpXOlsPbdc8YAc23LAq/huqMiD6UuixPBeXAbuj3lcAh0W6RGQ+UG6MeUpEEq9fHSTKhxVw83m2Z3p7Z4j3ag6wcW9j+E8Tz2+q5v9WHwrAlZbkMXVsMROGFzBhWAEThg9h/PACxg3NJ9eXQPfCdGfU1NRcN6fAKvHZF8PTX7MW+6rlMP086woqnd+368eNc0RjZcQx8Okn4L5z4f7zrVIfwISbw2iqgj99Ns6ZsQIf/akqc5eQsKNTRDzAncCVMZx7LXAtwPjx7oqE5/g8XdZ4BGMMNU1tbAgr+I17G9lS1cSr79XR0hHsOk8ESkvyGT+sgAnDCxg/vIAJw4YwYXgBI4tyKcjxMiTHh8ejPS2SwojJcPmfbM/wV39p25K+eJstVJp0qlXuk5dAcenhn6vbarNnAgMbJpByRk+33/v+8+CB8+Cqv9pWtQNh+4vwx6uhrckGtYeMsBlMvjzw59knIF+ufQqL3p9b7MyYR8UREna5iEgJ8B5wIPyRMcA+4Ly+3C5ucLkMFGMMNQfa2FXXzM66Znbua2ZX3cHwtpm6gz1nX+T7vQzJ9TEk1yr4Ibnh9+HXBTk+q/xzfeT7vRTkeCnI9VEQ/TrH27VOYa6PHJ/6+o+gZb9tFbztOdtArCncSmDUdKvYj15i2zI/9u+2gOmGHgZppxO734AHPm4t9Cufik/BhoJ2rOILt1qr/8L77Y1CcS2J+tB92KDoEqASGxS9xBjT46RTEXmBNPGhJ4um1g52hZX7vuZ2mtuCHGjrpLm9kwNtQZrbOznY1snBtiAH2zvtsbYgB9s6ae4IEgzF3qRqSI6XQEEOgQI/Q8PbyOuSfLsdOsRPSX4O+X4vuX4PeX4vuT4PuT772ucRJFM74hkD1RvDyv05O8My2G5bl4K14Jf9PrUyOsGOl22e/vBj4Monbd+c/jhQA49fbW9+sy+Cc++0vcgVV5OQD90Y0ykiNwDPYOcELTfGrBeR7wGrjDErnBU3/SnK8zOjtIQZpSVxf9YYQ3swREt7kIPtQVraO2luD3KwLUhLh33d3B6kua2TptZO6ls62N/cTn1zB/XN7eypb6G+xb6O9b7gEcj1ecnze8j1WaXv8wjGQMgYQgYMhlDIymeI2h82CEYU5lIWyKe0609e1/tRRbmpyxoSsRbn6Om2p377QZvquO05qwQjo/nSnaNOhIsfgoeXwf9+0tYB9FUs9f4r8NhnoLUePvZzO2sgU2/qWUS/FnqyyGQL3Q2EQias8NvZH1b2rR0h2jqDtEW2nSFaOw7ftnWEaO0M0hk0eDyCR0AAj1grXsTeAKLfGwM1TW3sqW9hT0ML9d360ns9wpjiPEoDeZQG8hk+JBe/V/B6ov6I4PUKPo/gEbv1eu2NJd/vpTDXR2GedTMV5voYkuujKM9Hrs+TuU8XA2HT0/Do5bYh22WPHdmMLRSymUErf2AzYy68Pz0GnitdJJrloqQhHo9QUuCnpMDPhOGDe+2DbZ3sbWihsr7VKvn6FirD27d21VN3oI2gMQRDhs6QSagNutcjXUq+MByfiLiU8vze8B9P+Akk+rU9LkDQGEKhQ/KEjCEYIry1fyKvI08rxtibZij8FNPTk0uO1xO+CfkpzPNRFL4RFYZvRtE3KcduTFOXwifutQHOP1xiUzL9eeEfpg7+dK19Opn5Sdu6NZ7RcIrrUYWuOM6QXB+TRxUxeVRsyiIUMl0KPqJUD7227qcDbZ0caLXxhgNtNgbRFN4XeR2JS7R2BGls7eh6ImntCD+BdIRoDw6sV74IeEXCTyb2CeXQkwrIYe/tvvbOEAfaOmOKiXg9Qo7Xg897aOv3eg577fN6yPEKPo+Hghwvxfl+SvL9FOf5KM73H3qffwrjT/4xZS98hY4/XIb34ofw7H0L/u8qO5v33Dth4b+riyUDUZeLklUEQ4b2sIuptTOIMVaZRtw8nigXkMdjlbg3gaCxMYbWjhBNbR0cbAtyoLWTpraOw25OTeGbUkcwREfQ0BEM0RnedoQMHZ0hOkMh2oP2dUcwREtHkIaWDhpbOmhq6+zxKecS7z/4of+3vBOaxAx5nz0ykm/5vsbOnMk2VuLzkBMOjkfe5/q95Hg95PjsTcTvtef4wzeTw9978PuEHK8Xv1fICa+XEz4n+rzItbwesbGdyJMNkacaE94XjteEn3J6+l7R++wKh37bts5Q2HUYPPS6y814yNXY1hEiGAp1PWEFTc9PXcHwE5dHbGyspOsmeuh15E9R3uCkJqvLRVHCeD1Cfo6X/JzBKQQTibpekrwboZChqa2TxpYOq+RbO2hs6aSxZTavbhnKh7b+mI1DT+PP479BuclnVHfl1hmivqWDto4g7eH37UF747A3EDPgJxs3E/2kJRK+iUc9dXnCN3qPCCFjaGzpoLOPpy0RKMoNPy3lWQVflGefoArzfF3vD9uG3W55Pm/XzTUnfIP1e+M3JFShK0qa4/FIl5V4RK3oov+CxquZVjSGaQm4WIwxXU8PHUGr8Ns77Z/I/rau9+Fj4XOj90fcT91dV0QUKxEFa19Hixyt3KK/SWS3RyT8lBH1xBHO2soLbyP7Ik8L8f4dRJ6MGlo6aGiO3EA7u/ZFbqpNrXZ/ZX0Lm1o7aAo/kcWTkgwcpuAjqcZ9oQpdUTKd4rEJLyEi5PgkqwvZRCRc/OdjbEn8fYsiN4Sm1k6awko+oujbOqOejrpcRaEe9/fVmEEVuqIoyiAQfUMYXZw34HV+cUnvx7L3dqsoipJhqEJXFEXJEFShK4qiZAiq0BVFUTIEVeiKoigZgip0RVGUDEEVuqIoSoagCl1RFCVDSFlzLhFpAjan5OKDywigNtVCDALZ8D2z4TuCfk+3M8EYM7KnA6msFN3cW8ewTEJEVun3zAyy4TuCfs90Rl0uiqIoGYIqdEVRlAwhlQr93hReezDR75k5ZMN3BP2eaUvKgqKKoiiKs6jLRVEUJUNIiUIXkbNFZLOIbBORb6RChsFARN4XkbUi8raIZMQAVRFZLiLVIrIuat8wEXlWRLaGt0NTKaMT9PI9bxaRyvDv+baILE2ljE4gIuUislJENojIehH5Unh/xvymfXzHzPs9B9vlIiJeYAtwBlABvAksM8ZsGFRBBgEReR9YaIxJx1zXHhGRk4ADwAPGmJnhfbcD+4wxPwrfoIcaY76eSjkTpZfveTNwwBjz41TK5iQiMhYYa4xZIyJFwGrg48CVZMhv2sd3/BQZ9numwkJfDGwzxmw3xrQDfwDOT4EcygAwxrwE7Ou2+3zg/vDr+7H/WdKaXr5nxmGM2WuMWRN+3QRsBMrIoN+0j++YcaRCoZcBu6PeV5Chf7mAAf4uIqtF5NpUC5NERhtj9oZffwCMTqUwSeYGEXk37JJJWzdET4jIRGAe8DoZ+pt2+46QYb+nBkWTywnGmPnAOcD14cf4jMZYH16mpk7dDRwNzAX2Aj9JrTjOISKFwB+BLxtjGqOPZcpv2sN3zLjfMxUKvRIoj3o/Lrwv4zDGVIa31cCfsO6mTKQq7KeM+CurUyxPUjDGVBljgsaYEPBrMuT3FBE/VtE9ZIx5PLw7o37Tnr5jJv6eqVDobwLHiMhRIpIDXAysSIEcSUVEhoQDMIjIEOBMYF3fn0pbVgBXhF9fATyRQlmSRkTBhbmADPg9rlT39AAAAL1JREFURUSA3wIbjTF3Rh3KmN+0t++Ykb9nKgqLwulBPwO8wHJjzA8GXYgkIyKTsFY52CZov8+E7ykiDwOnYDvVVQE3AX8GHgXGAzuBTxlj0jqg2Mv3PAX7eG6A94HPRvmZ0xIROQF4GVgLhMK7v4X1MWfEb9rHd1xGpv2eWimqKIqSGWhQVFEUJUNQha4oipIhqEJXFEXJEFShK4qiZAiq0BVFUTIEVeiKoigZgip0RVGUDEEVuqIoSobw/wGp+M/X9nc58AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history.history).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5160/5160 [==============================] - 0s 12us/sample - loss: 0.3258\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.32576462975306103"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Exercise solution](https://camo.githubusercontent.com/250388fde3fac9135ead9471733ee28e049f7a37/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f302f30362f46696c6f735f736567756e646f5f6c6f676f5f253238666c69707065642532392e6a7067)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1)\n",
    "Create an `my_mse()` function with two arguments: the true labels `y_true` and the model predictions `y_pred`. Make it return the mean squared error using TensorFlow operations. Note that you could write your own custom metrics in exactly the same way. **Tip**: recall that the MSE is the mean of the squares of prediction errors, which are the differences between the predictions and the labels, so you will need to use `tf.reduce_mean()` and `tf.square()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_mse(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_pred - y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2)\n",
    "Compile your model, passing it your custom loss function, then train it and evaluate it. **Tip**: don't forget to use the scaled sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=my_mse, optimizer=\"sgd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/10\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 2.3013 - val_loss: 1.3952\n",
      "Epoch 2/10\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 0.8092 - val_loss: 0.6777\n",
      "Epoch 3/10\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 0.6132 - val_loss: 0.5966\n",
      "Epoch 4/10\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 0.5610 - val_loss: 0.5658\n",
      "Epoch 5/10\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 0.5304 - val_loss: 0.5134\n",
      "Epoch 6/10\n",
      "11610/11610 [==============================] - 0s 34us/sample - loss: 0.5088 - val_loss: 0.4962\n",
      "Epoch 7/10\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.4929 - val_loss: 0.5083\n",
      "Epoch 8/10\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 0.4804 - val_loss: 0.4826\n",
      "Epoch 9/10\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.4704 - val_loss: 0.4828\n",
      "Epoch 10/10\n",
      "11610/11610 [==============================] - 0s 34us/sample - loss: 0.4622 - val_loss: 0.4846\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13e9ed0b8>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=10,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5160/5160 [==============================] - 0s 14us/sample - loss: 0.4569\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.45686613663222436"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3)\n",
    "Try building and compiling the model again, this time adding `\"mse\"` (or equivalently `\"mean_squared_error\"` or `keras.losses.mean_squared_error`) to the list of additional metrics, then train the model and make sure the `my_mse` is equal to the standard `mse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/10\n",
      "11610/11610 [==============================] - 0s 41us/sample - loss: 2.6145 - mean_squared_error: 2.6145 - val_loss: 2.9651 - val_mean_squared_error: 2.9651\n",
      "Epoch 2/10\n",
      "11610/11610 [==============================] - 0s 35us/sample - loss: 0.8185 - mean_squared_error: 0.8185 - val_loss: 1.0717 - val_mean_squared_error: 1.0717\n",
      "Epoch 3/10\n",
      "11610/11610 [==============================] - 0s 33us/sample - loss: 0.7278 - mean_squared_error: 0.7278 - val_loss: 0.6705 - val_mean_squared_error: 0.6705\n",
      "Epoch 4/10\n",
      "11610/11610 [==============================] - 0s 35us/sample - loss: 0.6809 - mean_squared_error: 0.6809 - val_loss: 0.7267 - val_mean_squared_error: 0.7267\n",
      "Epoch 5/10\n",
      "11610/11610 [==============================] - 0s 36us/sample - loss: 0.6535 - mean_squared_error: 0.6535 - val_loss: 0.6830 - val_mean_squared_error: 0.6830\n",
      "Epoch 6/10\n",
      "11610/11610 [==============================] - 0s 34us/sample - loss: 0.6305 - mean_squared_error: 0.6305 - val_loss: 0.5852 - val_mean_squared_error: 0.5852\n",
      "Epoch 7/10\n",
      "11610/11610 [==============================] - 0s 33us/sample - loss: 0.6055 - mean_squared_error: 0.6055 - val_loss: 0.6538 - val_mean_squared_error: 0.6538\n",
      "Epoch 8/10\n",
      "11610/11610 [==============================] - 0s 34us/sample - loss: 0.5914 - mean_squared_error: 0.5914 - val_loss: 0.5965 - val_mean_squared_error: 0.5965\n",
      "Epoch 9/10\n",
      "11610/11610 [==============================] - 0s 33us/sample - loss: 0.5731 - mean_squared_error: 0.5731 - val_loss: 0.6132 - val_mean_squared_error: 0.6132\n",
      "Epoch 10/10\n",
      "11610/11610 [==============================] - 0s 33us/sample - loss: 0.5579 - mean_squared_error: 0.5579 - val_loss: 0.5244 - val_mean_squared_error: 0.5244\n",
      "5160/5160 [==============================] - 0s 14us/sample - loss: 0.5441 - mean_squared_error: 0.5441\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5441172193187152, 0.5441173]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "model.compile(loss=my_mse, optimizer=\"sgd\", metrics=[\"mean_squared_error\"])\n",
    "model.fit(X_train_scaled, y_train, epochs=10,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4)\n",
    "If you want your code to be portable to other Python implementations of the Keras API, you should use the operations in `keras.backend` rather than TensorFlow operations directly. This package contains thin wrappers around the backend's operations (for example, `keras.backend.square()` simply calls `tf.square()`). Try reimplementing the `my_mse()` function this way and use it to train and evaluate your model again. **Tip**: people frequently define `K = keras.backend` to make their code more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_portable_mse(y_true, y_pred):\n",
    "    K = keras.backend\n",
    "    return K.mean(K.square(y_pred - y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/10\n",
      "11610/11610 [==============================] - 0s 42us/sample - loss: 1.9864 - mean_squared_error: 1.9864 - val_loss: 2.9839 - val_mean_squared_error: 2.9839\n",
      "Epoch 2/10\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 0.7440 - mean_squared_error: 0.7440 - val_loss: 0.7090 - val_mean_squared_error: 0.7090\n",
      "Epoch 3/10\n",
      "11610/11610 [==============================] - 0s 33us/sample - loss: 0.6614 - mean_squared_error: 0.6614 - val_loss: 0.6152 - val_mean_squared_error: 0.6152\n",
      "Epoch 4/10\n",
      "11610/11610 [==============================] - 0s 33us/sample - loss: 0.6243 - mean_squared_error: 0.6243 - val_loss: 0.5762 - val_mean_squared_error: 0.5762\n",
      "Epoch 5/10\n",
      "11610/11610 [==============================] - 0s 33us/sample - loss: 0.5936 - mean_squared_error: 0.5936 - val_loss: 0.5499 - val_mean_squared_error: 0.5499\n",
      "Epoch 6/10\n",
      "11610/11610 [==============================] - 0s 34us/sample - loss: 0.5677 - mean_squared_error: 0.5677 - val_loss: 0.5243 - val_mean_squared_error: 0.5243\n",
      "Epoch 7/10\n",
      "11610/11610 [==============================] - 0s 33us/sample - loss: 0.5455 - mean_squared_error: 0.5455 - val_loss: 0.5017 - val_mean_squared_error: 0.5017\n",
      "Epoch 8/10\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 0.5265 - mean_squared_error: 0.5265 - val_loss: 0.4844 - val_mean_squared_error: 0.4844\n",
      "Epoch 9/10\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 0.5100 - mean_squared_error: 0.5100 - val_loss: 0.4692 - val_mean_squared_error: 0.4692\n",
      "Epoch 10/10\n",
      "11610/11610 [==============================] - 0s 33us/sample - loss: 0.4963 - mean_squared_error: 0.4963 - val_loss: 0.4572 - val_mean_squared_error: 0.4572\n",
      "5160/5160 [==============================] - 0s 15us/sample - loss: 0.4914 - mean_squared_error: 0.4914\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4914473322472831, 0.49144742]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "model.compile(loss=my_portable_mse, optimizer=\"sgd\", metrics=[\"mean_squared_error\"])\n",
    "model.fit(X_train_scaled, y_train, epochs=10,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Exercise](https://c1.staticflickr.com/9/8101/8553474140_c50cf08708_b.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 – Custom layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1)\n",
    "Some layers have no weights, such as `keras.layers.Flatten` or `keras.layers.ReLU`. If you want to create a custom layer without any weights, the simplest option is to create a `keras.layers.Lambda` layer and pass it the function to perform. For example, try creating a custom layer that applies the softplus function (log(exp(X) + 1), and try calling this layer like a regular function.\n",
    "\n",
    "**Tip**: you can use `tf.math.softplus()` rather than computing the log and the exponential manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylayer = keras.layers.Lambda(function = tf.math.softplus)\n",
    "# or mylayer = keras.layers.Lambda(lambda x: tf.math.softplus(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "11610/11610 [==============================] - 0s 35us/sample - loss: 1.2735 - val_loss: 1.8961\n",
      "Epoch 2/100\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.5262 - val_loss: 0.4270\n",
      "Epoch 3/100\n",
      "11610/11610 [==============================] - 0s 30us/sample - loss: 0.4355 - val_loss: 0.3921\n",
      "Epoch 4/100\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.4087 - val_loss: 0.3728\n",
      "Epoch 5/100\n",
      "11610/11610 [==============================] - 0s 33us/sample - loss: 0.3942 - val_loss: 0.3620\n",
      "Epoch 6/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3859 - val_loss: 0.3594\n",
      "Epoch 7/100\n",
      "11610/11610 [==============================] - 0s 30us/sample - loss: 0.3785 - val_loss: 0.3774\n",
      "Epoch 8/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3742 - val_loss: 0.3553\n",
      "Epoch 9/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3708 - val_loss: 0.3455\n",
      "Epoch 10/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3640 - val_loss: 0.3944\n",
      "Epoch 11/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3625 - val_loss: 0.3376\n",
      "Epoch 12/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3585 - val_loss: 0.3511\n",
      "Epoch 13/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3566 - val_loss: 0.3332\n",
      "Epoch 14/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3548 - val_loss: 0.3306\n",
      "Epoch 15/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3517 - val_loss: 0.3970\n",
      "Epoch 16/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3489 - val_loss: 0.3260\n",
      "Epoch 17/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3474 - val_loss: 0.3278\n",
      "Epoch 18/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3448 - val_loss: 0.3224\n",
      "Epoch 19/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3415 - val_loss: 0.4023\n",
      "Epoch 20/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3482 - val_loss: 0.3194\n",
      "Epoch 21/100\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3413 - val_loss: 0.3189\n",
      "Epoch 22/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3388 - val_loss: 0.3202\n",
      "Epoch 23/100\n",
      "11610/11610 [==============================] - 0s 26us/sample - loss: 0.3375 - val_loss: 0.3192\n",
      "Epoch 24/100\n",
      "11610/11610 [==============================] - 0s 30us/sample - loss: 0.3360 - val_loss: 0.3149\n",
      "Epoch 25/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3333 - val_loss: 0.3118\n",
      "Epoch 26/100\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3310 - val_loss: 0.3213\n",
      "Epoch 27/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3329 - val_loss: 0.3136\n",
      "Epoch 28/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3313 - val_loss: 0.3146\n",
      "Epoch 29/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3306 - val_loss: 0.3215\n",
      "Epoch 30/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3303 - val_loss: 0.3134\n",
      "Epoch 31/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3303 - val_loss: 0.3862\n",
      "Epoch 32/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3304 - val_loss: 0.3124\n",
      "Epoch 33/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3275 - val_loss: 0.3066\n",
      "Epoch 34/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3247 - val_loss: 0.3040\n",
      "Epoch 35/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3236 - val_loss: 0.3045\n",
      "Epoch 36/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3223 - val_loss: 0.4378\n",
      "Epoch 37/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3247 - val_loss: 0.3033\n",
      "Epoch 38/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3225 - val_loss: 0.3061\n",
      "Epoch 39/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3201 - val_loss: 0.3015\n",
      "Epoch 40/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3221 - val_loss: 0.3146\n",
      "Epoch 41/100\n",
      "11610/11610 [==============================] - 0s 30us/sample - loss: 0.3199 - val_loss: 0.3021\n",
      "Epoch 42/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3201 - val_loss: 0.3001\n",
      "Epoch 43/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3184 - val_loss: 0.3395\n",
      "Epoch 44/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3196 - val_loss: 0.3024\n",
      "Epoch 45/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3197 - val_loss: 0.2975\n",
      "Epoch 46/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3168 - val_loss: 0.2993\n",
      "Epoch 47/100\n",
      "11610/11610 [==============================] - 0s 30us/sample - loss: 0.3154 - val_loss: 0.2988\n",
      "Epoch 48/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3143 - val_loss: 0.3216\n",
      "Epoch 49/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3143 - val_loss: 0.2970\n",
      "Epoch 50/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3152 - val_loss: 0.3958\n",
      "Epoch 51/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3126 - val_loss: 0.2973\n",
      "Epoch 52/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3170 - val_loss: 0.2948\n",
      "Epoch 53/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3132 - val_loss: 0.2968\n",
      "Epoch 54/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3110 - val_loss: 0.9568\n",
      "Epoch 55/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3136 - val_loss: 0.2931\n",
      "Epoch 56/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3105 - val_loss: 0.2912\n",
      "Epoch 57/100\n",
      "11610/11610 [==============================] - 0s 30us/sample - loss: 0.3114 - val_loss: 0.2942\n",
      "Epoch 58/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3127 - val_loss: 0.2920\n",
      "Epoch 59/100\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3113 - val_loss: 0.2997\n",
      "Epoch 60/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3084 - val_loss: 0.3321\n",
      "Epoch 61/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3099 - val_loss: 0.2932\n",
      "Epoch 62/100\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3095 - val_loss: 0.3970\n",
      "Epoch 63/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3107 - val_loss: 0.2928\n",
      "Epoch 64/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3139 - val_loss: 0.2926\n",
      "Epoch 65/100\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3101 - val_loss: 0.2906\n",
      "Epoch 66/100\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3082 - val_loss: 0.7097\n",
      "Epoch 67/100\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3103 - val_loss: 0.2915\n",
      "Epoch 68/100\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3099 - val_loss: 0.2898\n",
      "Epoch 69/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3083 - val_loss: 0.2887\n",
      "Epoch 70/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3056 - val_loss: 0.2911\n",
      "Epoch 71/100\n",
      "11610/11610 [==============================] - 0s 30us/sample - loss: 0.3055 - val_loss: 0.9099\n",
      "Epoch 72/100\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 0.3121 - val_loss: 0.2908\n",
      "Epoch 73/100\n",
      "11610/11610 [==============================] - 0s 30us/sample - loss: 0.3062 - val_loss: 0.2913\n",
      "Epoch 74/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3055 - val_loss: 0.2892\n",
      "Epoch 75/100\n",
      "11610/11610 [==============================] - 0s 30us/sample - loss: 0.3087 - val_loss: 0.2900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3038 - val_loss: 0.3101\n",
      "Epoch 77/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3046 - val_loss: 0.4532\n",
      "Epoch 78/100\n",
      "11610/11610 [==============================] - 0s 33us/sample - loss: 0.3071 - val_loss: 0.2933\n",
      "Epoch 79/100\n",
      "11610/11610 [==============================] - 0s 34us/sample - loss: 0.3059 - val_loss: 0.2898\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1),\n",
    "    keras.layers.Lambda(function = tf.math.softplus)\n",
    "])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "              loss=\"mse\")\n",
    "history = model.fit(X_train_scaled, y_train, validation_data = (X_valid_scaled, y_valid),\n",
    "          epochs = 100, \n",
    "          callbacks = [tf.keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f7cf33ff9b0>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history.history).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2)\n",
    "Create a regression model like in exercise 1, but add your softplus layer at the top (i.e., after the existing 1-unit dense layer). This can be useful to ensure that your model never predicts negative values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "11610/11610 [==============================] - 0s 36us/sample - loss: 1.3435 - val_loss: 1.6577\n",
      "Epoch 2/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.4695 - val_loss: 0.7623\n",
      "Epoch 3/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.4137 - val_loss: 0.4525\n",
      "Epoch 4/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3940 - val_loss: 0.4235\n",
      "Epoch 5/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3841 - val_loss: 0.4194\n",
      "Epoch 6/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3769 - val_loss: 0.4307\n",
      "Epoch 7/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3711 - val_loss: 0.3836\n",
      "Epoch 8/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3648 - val_loss: 0.5309\n",
      "Epoch 9/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3618 - val_loss: 0.3424\n",
      "Epoch 10/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3589 - val_loss: 0.3457\n",
      "Epoch 11/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3572 - val_loss: 0.5671\n",
      "Epoch 12/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3533 - val_loss: 0.3349\n",
      "Epoch 13/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3523 - val_loss: 0.3410\n",
      "Epoch 14/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3477 - val_loss: 0.3308\n",
      "Epoch 15/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3456 - val_loss: 0.3487\n",
      "Epoch 16/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3420 - val_loss: 0.3526\n",
      "Epoch 17/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3458 - val_loss: 0.3238\n",
      "Epoch 18/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3429 - val_loss: 0.3241\n",
      "Epoch 19/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3370 - val_loss: 0.4825\n",
      "Epoch 20/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3391 - val_loss: 0.3195\n",
      "Epoch 21/100\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3339 - val_loss: 0.3204\n",
      "Epoch 22/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3333 - val_loss: 0.3765\n",
      "Epoch 23/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3312 - val_loss: 0.3208\n",
      "Epoch 24/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3285 - val_loss: 0.3155\n",
      "Epoch 25/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3257 - val_loss: 0.3143\n",
      "Epoch 26/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3238 - val_loss: 0.3812\n",
      "Epoch 27/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3244 - val_loss: 0.3107\n",
      "Epoch 28/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3259 - val_loss: 0.3089\n",
      "Epoch 29/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3190 - val_loss: 0.3065\n",
      "Epoch 30/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3194 - val_loss: 0.3739\n",
      "Epoch 31/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3215 - val_loss: 0.3105\n",
      "Epoch 32/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3183 - val_loss: 0.3043\n",
      "Epoch 33/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3168 - val_loss: 0.3036\n",
      "Epoch 34/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3134 - val_loss: 0.4119\n",
      "Epoch 35/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3192 - val_loss: 0.3049\n",
      "Epoch 36/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3144 - val_loss: 0.3051\n",
      "Epoch 37/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3166 - val_loss: 0.3003\n",
      "Epoch 38/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3124 - val_loss: 0.3690\n",
      "Epoch 39/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3110 - val_loss: 0.2983\n",
      "Epoch 40/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3159 - val_loss: 0.2995\n",
      "Epoch 41/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3117 - val_loss: 0.2992\n",
      "Epoch 42/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3096 - val_loss: 0.3107\n",
      "Epoch 43/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3101 - val_loss: 0.2981\n",
      "Epoch 44/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3090 - val_loss: 0.2980\n",
      "Epoch 45/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3074 - val_loss: 0.2967\n",
      "Epoch 46/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3069 - val_loss: 0.2966\n",
      "Epoch 47/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3042 - val_loss: 0.2947\n",
      "Epoch 48/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3048 - val_loss: 0.2959\n",
      "Epoch 49/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3049 - val_loss: 0.2938\n",
      "Epoch 50/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3049 - val_loss: 0.2940\n",
      "Epoch 51/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3027 - val_loss: 0.2946\n",
      "Epoch 52/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3048 - val_loss: 0.3166\n",
      "Epoch 53/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3035 - val_loss: 0.2918\n",
      "Epoch 54/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3037 - val_loss: 0.2987\n",
      "Epoch 55/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3001 - val_loss: 0.2929\n",
      "Epoch 56/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3014 - val_loss: 0.4566\n",
      "Epoch 57/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3030 - val_loss: 0.2924\n",
      "Epoch 58/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3028 - val_loss: 0.3008\n",
      "Epoch 59/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3111 - val_loss: 0.2921\n",
      "Epoch 60/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3029 - val_loss: 0.2902\n",
      "Epoch 61/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3007 - val_loss: 0.3626\n",
      "Epoch 62/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3003 - val_loss: 0.2906\n",
      "Epoch 63/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.2972 - val_loss: 0.2864\n",
      "Epoch 64/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.2972 - val_loss: 0.2890\n",
      "Epoch 65/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3011 - val_loss: 0.2870\n",
      "Epoch 66/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.2987 - val_loss: 0.3112\n",
      "Epoch 67/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.2965 - val_loss: 0.2881\n",
      "Epoch 68/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.2963 - val_loss: 0.2886\n",
      "Epoch 69/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.2953 - val_loss: 0.2880\n",
      "Epoch 70/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.2975 - val_loss: 0.2860\n",
      "Epoch 71/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.2943 - val_loss: 0.2849\n",
      "Epoch 72/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.2963 - val_loss: 0.2862\n",
      "Epoch 73/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.2936 - val_loss: 0.3195\n",
      "Epoch 74/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3012 - val_loss: 0.2863\n",
      "Epoch 75/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.2975 - val_loss: 0.2878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.2965 - val_loss: 0.2866\n",
      "Epoch 77/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3013 - val_loss: 0.3356\n",
      "Epoch 78/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.2954 - val_loss: 0.2853\n",
      "Epoch 79/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.2952 - val_loss: 0.2865\n",
      "Epoch 80/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.2954 - val_loss: 0.4342\n",
      "Epoch 81/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.2958 - val_loss: 0.2855\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Lambda(function = tf.math.softplus),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "              loss=\"mse\")\n",
    "history = model.fit(X_train_scaled, y_train, validation_data = (X_valid_scaled, y_valid),\n",
    "          epochs = 100, \n",
    "          callbacks = [tf.keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3)\n",
    "Alternatively, try using this softplus layer as the activation function of the output layer.\n",
    "\n",
    "**Notes**:\n",
    "* setting a layer's activation function is just a handy way of adding an extra weightless layer.\n",
    "* Keras supports the softplus activation function out of the box:\n",
    "  * set `activation=\"softplus\"`\n",
    "  * or set `activation=keras.activations.softplus`\n",
    "  * or add a `keras.layers.Activation(\"softplus\")` layer to your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "11610/11610 [==============================] - 0s 35us/sample - loss: 1.1410 - val_loss: 0.6002\n",
      "Epoch 2/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.5143 - val_loss: 0.4324\n",
      "Epoch 3/100\n",
      "11610/11610 [==============================] - 0s 30us/sample - loss: 0.4300 - val_loss: 0.4575\n",
      "Epoch 4/100\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.4006 - val_loss: 0.3655\n",
      "Epoch 5/100\n",
      "11610/11610 [==============================] - 0s 30us/sample - loss: 0.3874 - val_loss: 0.3940\n",
      "Epoch 6/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3794 - val_loss: 0.4076\n",
      "Epoch 7/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3772 - val_loss: 0.4717\n",
      "Epoch 8/100\n",
      "11610/11610 [==============================] - 0s 30us/sample - loss: 0.3693 - val_loss: 0.4774\n",
      "Epoch 9/100\n",
      "11610/11610 [==============================] - 0s 30us/sample - loss: 0.3663 - val_loss: 0.3475\n",
      "Epoch 10/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3615 - val_loss: 0.3541\n",
      "Epoch 11/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3598 - val_loss: 0.3989\n",
      "Epoch 12/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3550 - val_loss: 0.4824\n",
      "Epoch 13/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3544 - val_loss: 0.3319\n",
      "Epoch 14/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3493 - val_loss: 0.3821\n",
      "Epoch 15/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3478 - val_loss: 0.4374\n",
      "Epoch 16/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3466 - val_loss: 0.3311\n",
      "Epoch 17/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3442 - val_loss: 0.3208\n",
      "Epoch 18/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3393 - val_loss: 0.6687\n",
      "Epoch 19/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3417 - val_loss: 0.3222\n",
      "Epoch 20/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3350 - val_loss: 0.3189\n",
      "Epoch 21/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3342 - val_loss: 0.3880\n",
      "Epoch 22/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3303 - val_loss: 0.3137\n",
      "Epoch 23/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3298 - val_loss: 0.3130\n",
      "Epoch 24/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3265 - val_loss: 0.4655\n",
      "Epoch 25/100\n",
      "11610/11610 [==============================] - 0s 26us/sample - loss: 0.3274 - val_loss: 0.3154\n",
      "Epoch 26/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3252 - val_loss: 0.3068\n",
      "Epoch 27/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3269 - val_loss: 0.4838\n",
      "Epoch 28/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3217 - val_loss: 0.3101\n",
      "Epoch 29/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3199 - val_loss: 0.4303\n",
      "Epoch 30/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3215 - val_loss: 0.3050\n",
      "Epoch 31/100\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 0.3207 - val_loss: 0.3091\n",
      "Epoch 32/100\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3187 - val_loss: 0.6245\n",
      "Epoch 33/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3185 - val_loss: 0.3036\n",
      "Epoch 34/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3170 - val_loss: 0.3027\n",
      "Epoch 35/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3160 - val_loss: 0.3315\n",
      "Epoch 36/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3161 - val_loss: 0.4814\n",
      "Epoch 37/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3139 - val_loss: 0.3265\n",
      "Epoch 38/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3129 - val_loss: 0.4644\n",
      "Epoch 39/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3143 - val_loss: 1.5678\n",
      "Epoch 40/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3123 - val_loss: 0.3011\n",
      "Epoch 41/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3134 - val_loss: 0.3058\n",
      "Epoch 42/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3110 - val_loss: 0.2967\n",
      "Epoch 43/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3097 - val_loss: 0.4850\n",
      "Epoch 44/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3114 - val_loss: 0.3010\n",
      "Epoch 45/100\n",
      "11610/11610 [==============================] - 0s 30us/sample - loss: 0.3101 - val_loss: 0.3017\n",
      "Epoch 46/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3078 - val_loss: 0.3118\n",
      "Epoch 47/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3072 - val_loss: 0.2993\n",
      "Epoch 48/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3060 - val_loss: 0.9228\n",
      "Epoch 49/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3069 - val_loss: 0.2970\n",
      "Epoch 50/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3065 - val_loss: 0.3035\n",
      "Epoch 51/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3073 - val_loss: 0.2945\n",
      "Epoch 52/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3062 - val_loss: 0.5509\n",
      "Epoch 53/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3083 - val_loss: 0.2948\n",
      "Epoch 54/100\n",
      "11610/11610 [==============================] - 0s 26us/sample - loss: 0.3040 - val_loss: 0.2935\n",
      "Epoch 55/100\n",
      "11610/11610 [==============================] - 0s 30us/sample - loss: 0.3033 - val_loss: 0.4730\n",
      "Epoch 56/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3053 - val_loss: 0.9833\n",
      "Epoch 57/100\n",
      "11610/11610 [==============================] - 0s 30us/sample - loss: 0.3071 - val_loss: 0.2950\n",
      "Epoch 58/100\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 0.3030 - val_loss: 0.2955\n",
      "Epoch 59/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3055 - val_loss: 0.3160\n",
      "Epoch 60/100\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3037 - val_loss: 0.7201\n",
      "Epoch 61/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3029 - val_loss: 0.2928\n",
      "Epoch 62/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3016 - val_loss: 0.6561\n",
      "Epoch 63/100\n",
      "11610/11610 [==============================] - 0s 30us/sample - loss: 0.3012 - val_loss: 0.2918\n",
      "Epoch 64/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3011 - val_loss: 0.4750\n",
      "Epoch 65/100\n",
      "11610/11610 [==============================] - 0s 30us/sample - loss: 0.2998 - val_loss: 0.2953\n",
      "Epoch 66/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3002 - val_loss: 0.2922\n",
      "Epoch 67/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3020 - val_loss: 1.1506\n",
      "Epoch 68/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.2995 - val_loss: 0.2909\n",
      "Epoch 69/100\n",
      "11610/11610 [==============================] - 0s 30us/sample - loss: 0.2996 - val_loss: 0.2924\n",
      "Epoch 70/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3005 - val_loss: 0.3848\n",
      "Epoch 71/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3006 - val_loss: 0.2900\n",
      "Epoch 72/100\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.2977 - val_loss: 1.0294\n",
      "Epoch 73/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3035 - val_loss: 0.2874\n",
      "Epoch 74/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.2996 - val_loss: 0.2966\n",
      "Epoch 75/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3015 - val_loss: 0.2914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.2971 - val_loss: 0.2898\n",
      "Epoch 77/100\n",
      "11610/11610 [==============================] - 0s 33us/sample - loss: 0.2969 - val_loss: 0.3681\n",
      "Epoch 78/100\n",
      "11610/11610 [==============================] - 0s 34us/sample - loss: 0.2976 - val_loss: 0.2912\n",
      "Epoch 79/100\n",
      "11610/11610 [==============================] - 0s 30us/sample - loss: 0.2992 - val_loss: 0.2882\n",
      "Epoch 80/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.2982 - val_loss: 0.3757\n",
      "Epoch 81/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.2956 - val_loss: 0.6016\n",
      "Epoch 82/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.2977 - val_loss: 0.2895\n",
      "Epoch 83/100\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.2990 - val_loss: 0.2908\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1, activation=\"softplus\"),\n",
    "])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "              loss=\"mse\")\n",
    "history = model.fit(X_train_scaled, y_train, validation_data = (X_valid_scaled, y_valid),\n",
    "          epochs = 100, \n",
    "          callbacks = [tf.keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4)\n",
    "Now let's create a custom layer with its own weights. Use the following template to create a `MyDense` layer that computes $\\phi(\\mathbf{X} \\mathbf{W}) + \\mathbf{b}$, where $\\phi$ is the (optional) activation function, $\\mathbf{X}$ is the input data, $\\mathbf{W}$ represents the kernel (i.e., connection weights), and $\\mathbf{b}$ represents the biases, then train and evaluate a model using this instead of a regular `Dense` layer.\n",
    "\n",
    "**Tips**:\n",
    "* The constructor `__init__()`:\n",
    "  * It must have all your layer's hyperparameters as arguments, and save them to instance variables. You will need the number of `units` and the optional `activation` function. To support all kinds of activation functions (strings or functions), simply create a `keras.layers.Activation` passing it the `activation` argument.\n",
    "  * The `**kwargs` argument must be passed to the base class's constructor (`super().__init__()`) so your class can support the `input_shape` argument, and more.\n",
    "* The `build()` method:\n",
    "  * The `build()` method will be called automatically by Keras when it knows the shape of the inputs. Note that the argument should really be called `batch_input_shape` since it includes the batch size.\n",
    "  * You must call `self.add_weight()` for each weight you want to create, specifying its `name`, `shape` (which often depends on the `input_shape`), how to initialize it, and whether or not it is `trainable`. You need two weights: the `kernel` (connection weights) and the `biases`. The kernel must be initialized randomly. The biases are usually initialized with zeros. **Note**: you can find many initializers in `keras.initializers`.\n",
    "  * Do not forget to call `super().build()`, so Keras knows that the model has been built.\n",
    "  * Note: you could create the weights in the constructor, but it is preferable to create them in the `build()` method, because users of your class may not always know the `input_shape` when creating the model. The first time the model is used on some actual data, the `build()` method will automatically be called with the actual `input_shape`.\n",
    "* The `call()` method:\n",
    "  * This is where to code your layer's actual computations. As before, you can use TensorFlow operations directly, or use `keras.backend` operations if you want the layer to be portable to other Keras implementations.\n",
    "* The `compute_output_shape()` method:\n",
    "  * You do not need to implement this method when using tf.keras, as the `Layer` class provides a good implementation.\n",
    "  * However, if want to port your code to another Keras implementation (such as keras-team), and if the output shape is different from the input shape, then you need to implement this method. Note that the input shape is actually the batch input shape, and the ouptut shape must be the batch output shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This template was copied from https://keras.io/layers/writing-your-own-keras-layers/\n",
    "# I just removed the imports and replaced Layer with keras.layers.Layer.\n",
    "\n",
    "class MyLayer(keras.layers.Layer):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.kernel = self.add_weight(name='kernel', \n",
    "                                      shape=(input_shape[1], self.output_dim),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        super(MyLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x):\n",
    "        return K.dot(x, self.kernel)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Exercise solution](https://camo.githubusercontent.com/250388fde3fac9135ead9471733ee28e049f7a37/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f302f30362f46696c6f735f736567756e646f5f6c6f676f5f253238666c69707065642532392e6a7067)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 – Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1)\n",
    "Some layers have no weights, such as `keras.layers.Flatten` or `keras.layers.ReLU`. If you want to create a custom layer without any weights, the simplest option is to create a `keras.layers.Lambda` layer and pass it the function to perform. For example, try creating a custom layer that applies the softplus function (log(exp(X) + 1), and try calling this layer like a regular function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_softplus = keras.layers.Lambda(lambda X: tf.nn.softplus(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=107512, shape=(5,), dtype=float32, numpy=\n",
       "array([4.5417706e-05, 6.7153489e-03, 6.9314718e-01, 5.0067153e+00,\n",
       "       1.0000046e+01], dtype=float32)>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_softplus([-10., -5., 0., 5., 10.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2)\n",
    "Create a regression model like in exercise 1, but add your softplus layer at the top (i.e., after the existing 1-unit dense layer). This can be useful to ensure that your model never predicts negative values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/10\n",
      "11610/11610 [==============================] - 0s 38us/sample - loss: 2.1855 - val_loss: 1.6445\n",
      "Epoch 2/10\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 1.0730 - val_loss: 0.8504\n",
      "Epoch 3/10\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 0.8289 - val_loss: 0.7364\n",
      "Epoch 4/10\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 0.7531 - val_loss: 0.6832\n",
      "Epoch 5/10\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 0.7082 - val_loss: 0.6478\n",
      "Epoch 6/10\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.6745 - val_loss: 0.6196\n",
      "Epoch 7/10\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.6468 - val_loss: 0.5953\n",
      "Epoch 8/10\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 0.6221 - val_loss: 0.5733\n",
      "Epoch 9/10\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 0.5997 - val_loss: 0.5536\n",
      "Epoch 10/10\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.5795 - val_loss: 0.5349\n",
      "5160/5160 [==============================] - 0s 14us/sample - loss: 0.5649\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.564937449148459"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1),\n",
    "    my_softplus\n",
    "])\n",
    "model.compile(loss=my_portable_mse, optimizer=\"sgd\")\n",
    "model.fit(X_train_scaled, y_train, epochs=10,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3)\n",
    "Alternatively, try using this softplus layer as the activation function of the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/10\n",
      "11610/11610 [==============================] - 0s 38us/sample - loss: 1.9194 - val_loss: 1.8773\n",
      "Epoch 2/10\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.8180 - val_loss: 0.7519\n",
      "Epoch 3/10\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.6807 - val_loss: 0.6417\n",
      "Epoch 4/10\n",
      "11610/11610 [==============================] - 0s 34us/sample - loss: 0.6313 - val_loss: 0.6009\n",
      "Epoch 5/10\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.5954 - val_loss: 0.5693\n",
      "Epoch 6/10\n",
      "11610/11610 [==============================] - 0s 34us/sample - loss: 0.5669 - val_loss: 0.5437\n",
      "Epoch 7/10\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 0.5440 - val_loss: 0.5220\n",
      "Epoch 8/10\n",
      "11610/11610 [==============================] - 0s 34us/sample - loss: 0.5247 - val_loss: 0.5045\n",
      "Epoch 9/10\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 0.5087 - val_loss: 0.4885\n",
      "Epoch 10/10\n",
      "11610/11610 [==============================] - 0s 34us/sample - loss: 0.4952 - val_loss: 0.4744\n",
      "5160/5160 [==============================] - 0s 14us/sample - loss: 0.4768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.47681303236835687"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1, activation=my_softplus)\n",
    "#   A few alternatives...\n",
    "#   keras.layers.Dense(1, activation=tf.function(lambda X: my_softplus(X)))\n",
    "#   keras.layers.Dense(1, activation=\"softplus\")\n",
    "#   keras.layers.Dense(1, activation=keras.activations.softplus)\n",
    "#   keras.layers.Dense(1), keras.layers.Activation(\"softplus\")\n",
    "])\n",
    "\n",
    "model.compile(loss=my_portable_mse, optimizer=\"sgd\")\n",
    "model.fit(X_train_scaled, y_train, epochs=10,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4)\n",
    "Now let's create a custom layer with its own weights. Use the following template to create a `MyDense` layer that computes $\\phi(\\mathbf{X} \\mathbf{W}) + \\mathbf{b}$, where $\\phi$ is the (optional) activation function, $\\mathbf{X}$ is the input data, $\\mathbf{W}$ represents the kernel (i.e., connection weights), and $\\mathbf{b}$ represents the biases, then train and evaluate a model using this instead of a regular `Dense` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDense(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        self.units = units\n",
    "        self.activation = keras.layers.Activation(activation)\n",
    "        super(MyDense, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # input_shape is actually batch input shape\n",
    "        self.kernel = self.add_weight(name='kernel', \n",
    "                                      shape=(input_shape[1], self.units),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        self.biases = self.add_weight(name='bias', \n",
    "                                      shape=(self.units,),\n",
    "                                      initializer='zeros',\n",
    "                                      trainable=True)\n",
    "        super(MyDense, self).build(input_shape)\n",
    "\n",
    "    def call(self, X):\n",
    "        return self.activation(X @ self.kernel + self.biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    MyDense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    MyDense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/10\n",
      "11610/11610 [==============================] - 1s 48us/sample - loss: 3.5886 - val_loss: 2.1575\n",
      "Epoch 2/10\n",
      "11610/11610 [==============================] - 0s 35us/sample - loss: 1.7396 - val_loss: 1.3169\n",
      "Epoch 3/10\n",
      "11610/11610 [==============================] - 0s 35us/sample - loss: 1.1695 - val_loss: 0.9721\n",
      "Epoch 4/10\n",
      "11610/11610 [==============================] - 0s 35us/sample - loss: 0.8931 - val_loss: 0.7806\n",
      "Epoch 5/10\n",
      "11610/11610 [==============================] - 0s 34us/sample - loss: 0.7530 - val_loss: 0.6845\n",
      "Epoch 6/10\n",
      "11610/11610 [==============================] - 0s 35us/sample - loss: 0.6859 - val_loss: 0.6331\n",
      "Epoch 7/10\n",
      "11610/11610 [==============================] - 0s 34us/sample - loss: 0.6483 - val_loss: 0.6010\n",
      "Epoch 8/10\n",
      "11610/11610 [==============================] - 0s 35us/sample - loss: 0.6222 - val_loss: 0.5775\n",
      "Epoch 9/10\n",
      "11610/11610 [==============================] - 0s 34us/sample - loss: 0.6013 - val_loss: 0.5575\n",
      "Epoch 10/10\n",
      "11610/11610 [==============================] - 0s 34us/sample - loss: 0.5828 - val_loss: 0.5397\n",
      "5160/5160 [==============================] - 0s 16us/sample - loss: 0.5635\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5634679314702056"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=\"sgd\")\n",
    "model.fit(X_train_scaled, y_train, epochs=10,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Exercise](https://c1.staticflickr.com/9/8101/8553474140_c50cf08708_b.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 – TensorFlow Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1)\n",
    "Examine and run the following code examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_elu(z, scale=1.0, alpha=1.0):\n",
    "    is_positive = tf.greater_equal(z, 0.0)\n",
    "    return scale * tf.where(is_positive, z, alpha * tf.nn.elu(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1032401, shape=(), dtype=float32, numpy=-0.95021296>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_elu(tf.constant(-3.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1032411, shape=(2,), dtype=float32, numpy=array([-0.95021296,  2.5       ], dtype=float32)>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_elu(tf.constant([-3., 2.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.def_function.Function at 0x7f7cf413b400>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_elu_tf = tf.function(scaled_elu)\n",
    "scaled_elu_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1032427, shape=(), dtype=float32, numpy=-0.95021296>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_elu_tf(tf.constant(-3.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1032442, shape=(2,), dtype=float32, numpy=array([-0.95021296,  2.5       ], dtype=float32)>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_elu_tf(tf.constant([-3., 2.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_elu_tf.python_function is scaled_elu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.61 ms ± 48.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit scaled_elu(tf.random.normal((1000, 1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.73 ms ± 54.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit scaled_elu_tf(tf.random.normal((1000, 1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_tf_code(func):\n",
    "    from IPython.display import display, Markdown\n",
    "    if hasattr(func, \"python_function\"):\n",
    "        func = func.python_function\n",
    "    code = tf.autograph.to_code(func, experimental_optional_features=None)\n",
    "    display(Markdown('```python\\n{}\\n```'.format(code)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "def tf__scaled_elu(z, scale=None, alpha=None):\n",
       "  do_return = False\n",
       "  retval_ = ag__.UndefinedReturnValue()\n",
       "  is_positive = ag__.converted_call('greater_equal', tf, ag__.STD, (z, 0.0), None)\n",
       "  do_return = True\n",
       "  retval_ = scale * ag__.converted_call('where', tf, ag__.STD, (is_positive, z, alpha * ag__.converted_call('elu', tf.nn, ag__.STD, (z,), None)), None)\n",
       "  do_return,\n",
       "  return ag__.retval(retval_)\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_tf_code(scaled_elu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = tf.Variable(0)\n",
    "\n",
    "@tf.function\n",
    "def add_21():\n",
    "    return var.assign_add(21)\n",
    "\n",
    "@tf.function\n",
    "def times_2():\n",
    "    return var.assign(var * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1049511, shape=(), dtype=int32, numpy=42>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_21()\n",
    "times_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def times_4(x):\n",
    "    return 4. * x\n",
    "\n",
    "@tf.function\n",
    "def times_4_plus_22(x):\n",
    "    return times_4(x) + 22."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1049523, shape=(), dtype=float32, numpy=42.0>"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times_4_plus_22(tf.constant(5.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute 1 + 1/2 + 1/4 + ...: the order of execution of the operations with side-effects (e.g., `assign()`) is preserved (in TF 1.x, `tf.control_dependencies()` was needed in such cases):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1049606, shape=(), dtype=float32, numpy=1.9999981>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = tf.Variable(0.)\n",
    "increment = tf.Variable(1.)\n",
    "\n",
    "@tf.function\n",
    "def converge_to_2(n_iterations):\n",
    "    for i in tf.range(n_iterations):\n",
    "        total.assign_add(increment)\n",
    "        increment.assign(increment / 2.0)\n",
    "    return total\n",
    "\n",
    "converge_to_2(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2)\n",
    "Write a function that computes the sum of squares from 1 to n, where n is an argument. Convert it to a graph function by using `tf.function` as a decorator. Display the code generated by autograph using the `display_tf_code()` function. Use `%timeit` to see how must faster the TensorFlow `Function` is compared to the Python function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3)\n",
    "Examine and run the following code examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def square(x):\n",
    "    tf.print(\"Calling\", x)  # part of the TF Function\n",
    "    print(\"Tracing\")  # NOT part of the TF Function\n",
    "    return tf.square(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing\n",
      "Calling 0\n",
      "Calling 1\n",
      "Calling 2\n",
      "Calling 3\n",
      "Calling 4\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    square(tf.constant(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing\n",
      "Calling 0\n",
      "Calling 1\n",
      "Calling 2\n",
      "Calling 3\n",
      "Calling 4\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    square(tf.constant(i, dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing\n",
      "Calling [0 0]\n",
      "Calling [1 1]\n",
      "Calling [2 2]\n",
      "Calling [3 3]\n",
      "Calling [4 4]\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    square(tf.constant([i, i], dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing\n",
      "Calling 0\n",
      "Tracing\n",
      "Calling 1\n",
      "Tracing\n",
      "Calling 2\n",
      "Tracing\n",
      "Calling 3\n",
      "Tracing\n",
      "Calling 4\n"
     ]
    }
   ],
   "source": [
    "# WARNING: when passing non-tensor values, a trace happens for any new value!\n",
    "# This is to allow optimization in case this value determines e.g., number of layers.\n",
    "for i in range(5):\n",
    "    square(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4)\n",
    "When you give Keras a custom loss function, it actually creates a graph function based on it, and then uses that graph function during training. The same is true of custom metric functions, and the `call()` method of custom layers and models. Create a `my_mse()` function, like you did earlier, but add an instruction to log a message inside it (use `print()`, *not* `tf.print()`!), and verify that the message is only logged once when you compile and train the model. Optionally, you can also find out when Keras converts custom metrics, layers and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5)\n",
    "Examine the following function, and try to call it with various argument types and shapes. Notice that only tensors of type `int32` and one dimension (of any size) are accepted now that we have specified the `input_signature`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(input_signature=[tf.TensorSpec([None], tf.int32, name=\"x\")])\n",
    "def cube(z):\n",
    "    return tf.pow(z, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Exercise solution](https://camo.githubusercontent.com/250388fde3fac9135ead9471733ee28e049f7a37/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f302f30362f46696c6f735f736567756e646f5f6c6f676f5f253238666c69707065642532392e6a7067)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 – Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1)\n",
    "Examine the code examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2)\n",
    "Write a function that computes the sum of squares from 1 to n, where n is an argument. Convert it to a graph function by using `tf.function` as a decorator. Display the code generated by autograph using the `display_tf_code()` function. Use `%timeit` to see how must faster the TensorFlow `Function` is compared to the Python function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def sum_squares(n):\n",
    "    s = tf.constant(0)\n",
    "    for i in range(1, n + 1):\n",
    "        s = s + i ** 2\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=212810, shape=(), dtype=int32, numpy=55>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_squares(tf.constant(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "from __future__ import print_function\n",
       "\n",
       "def tf__sum_squares(n):\n",
       "  do_return = False\n",
       "  retval_ = None\n",
       "  s = ag__.converted_call('constant', tf, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (0,), {})\n",
       "\n",
       "  def loop_body(loop_vars, s_1):\n",
       "    i = loop_vars\n",
       "    s_1 = s_1 + i ** 2\n",
       "    return s_1,\n",
       "  s, = ag__.for_stmt(ag__.converted_call(range, None, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (1, n + 1), {}), None, loop_body, (s,))\n",
       "  do_return = True\n",
       "  retval_ = s\n",
       "  return retval_\n",
       "\n",
       "\n",
       "\n",
       "tf__sum_squares.autograph_info__ = {}\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_tf_code(sum_squares.python_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274 µs ± 90.9 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sum_squares(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "358 ms ± 16.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sum_squares.python_function(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3)\n",
    "Examine the code examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4)\n",
    "When you give Keras a custom loss function, it actually creates a graph function based on it, and then uses that graph function during training. The same is true of custom metric functions, and the `call()` method of custom layers and models. Create a `my_mse()` function, like you did earlier, but add an instruction to log a message inside it (use `print()`, *not* `tf.print()`!), and verify that the message is only logged once when you compile and train the model. Optionally, you can also find out when Keras converts custom metrics, layers and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss function\n",
    "def my_mse(y_true, y_pred):\n",
    "    print(\"Tracing loss my_mse()\")\n",
    "    return tf.reduce_mean(tf.square(y_pred - y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom metric function\n",
    "def my_mae(y_true, y_pred):\n",
    "    print(\"Tracing metric my_mae()\")\n",
    "    return tf.reduce_mean(tf.abs(y_pred - y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom layer\n",
    "class MyDense(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        self.units = units\n",
    "        self.activation = keras.layers.Activation(activation)\n",
    "        super(MyDense, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(name='kernel', \n",
    "                                      shape=(input_shape[1], self.units),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        self.biases = self.add_weight(name='bias', \n",
    "                                      shape=(self.units,),\n",
    "                                      initializer='zeros',\n",
    "                                      trainable=True)\n",
    "        super(MyDense, self).build(input_shape)\n",
    "\n",
    "    def call(self, X):\n",
    "        print(\"Tracing MyDense.call()\")\n",
    "        return self.activation(X @ self.kernel + self.biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom model\n",
    "class MyModel(keras.models.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.hidden1 = MyDense(30, activation=\"relu\")\n",
    "        self.hidden2 = MyDense(30, activation=\"relu\")\n",
    "        self.output_ = MyDense(1)\n",
    "\n",
    "    def call(self, input):\n",
    "        print(\"Tracing MyModel.call()\")\n",
    "        hidden1 = self.hidden1(input)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input, hidden2])\n",
    "        output = self.output_(concat)\n",
    "        return output\n",
    "\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=my_mse, optimizer=\"sgd\", metrics=[my_mae])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing MyModel.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing metric my_mae()\n",
      "Tracing metric my_mae()\n",
      "Tracing loss my_mse()\n",
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/2\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 3.3322 - my_mae: 1.4788 - val_loss: 1.9528 - val_my_mae: 0.9931\n",
      "Epoch 2/2\n",
      "11610/11610 [==============================] - 0s 36us/sample - loss: 1.2325 - my_mae: 0.7870 - val_loss: 1.3645 - val_my_mae: 0.6417\n",
      "5160/5160 [==============================] - 0s 15us/sample - loss: 0.8552 - my_mae: 0.6404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8551779761794925, 0.640441]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that each custom function is traced just once, except for the metric function. That's a bit odd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5)\n",
    "Examine the following function, and try to call it with various argument types and shapes. Notice that only tensors of type `int32` and one dimension (of any size) are accepted now that we have specified the `input_signature`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(input_signature=[tf.TensorSpec([None], tf.int32, name=\"x\")])\n",
    "def cube(z):\n",
    "    return tf.pow(z, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=401992, shape=(3,), dtype=int32, numpy=array([ 1,  8, 27], dtype=int32)>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cube(tf.constant([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=401995, shape=(5,), dtype=int32, numpy=array([  1,   8,  27,  64, 125], dtype=int32)>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cube(tf.constant([1, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cube([1, 2, 3])\n",
    "except ValueError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python inputs incompatible with input_signature: inputs ((<tf.Tensor: id=401999, shape=(3,), dtype=float32, numpy=array([1., 2., 3.], dtype=float32)>,)), input_signature ((TensorSpec(shape=(None,), dtype=tf.int32, name='x'),))\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    cube(tf.constant([1., 2., 3]))\n",
    "except ValueError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python inputs incompatible with input_signature: inputs ((<tf.Tensor: id=402001, shape=(2, 2), dtype=int32, numpy=\n",
      "array([[1, 2],\n",
      "       [3, 4]], dtype=int32)>,)), input_signature ((TensorSpec(shape=(None,), dtype=tf.int32, name='x'),))\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    cube(tf.constant([[1, 2], [3, 4]]))\n",
    "except ValueError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Exercise](https://c1.staticflickr.com/9/8101/8553474140_c50cf08708_b.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 – Function Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1)\n",
    "Examine and run the following code examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(input_signature=[tf.TensorSpec([None], tf.int32, name=\"x\")])\n",
    "def cube(z):\n",
    "    return tf.pow(z, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.function.ConcreteFunction at 0x13fd37160>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cube_func_int32 = cube.get_concrete_function(tf.TensorSpec([None], tf.int32))\n",
    "cube_func_int32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cube_func_int32 is cube.get_concrete_function(tf.TensorSpec([5], tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cube_func_int32 is cube.get_concrete_function(tf.constant([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.framework.func_graph.FuncGraph at 0x14d5ffc88>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cube_func_int32.graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2)\n",
    "The function's graph is represented on the following diagram. Call the graph's `get_operations()` method to get the list of operations. Each operation has an `inputs` attribute that returns an iterator over its input tensors (these are symbolic: contrary to tensors we have used up to now, they have no value). It also has an `outputs` attribute that returns the list of output tensors. Each tensor has an `op` attribute that returns the operation it comes from. Try navigating through the graph using these methods and attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/cube_graph.png\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3)\n",
    "Each operation has a default name, such as `\"pow\"` (you can override it by setting the `name` attribute when you call the operation). In case of a name conflict, TensorFlow adds an underscore and anindex to make the name unique (e.g. `\"pow_1\"`). Moreover, each tensor has the same name as the operation that outputs it, followed by a colon `:` and the tensor's `index` (e.g., `\"pow:0\"`). Most operations have a single output tensor, so most tensors have a name that ends with `:0`. Try using `get_operation_by_name()` and `get_tensor_by_name()` to access any op and tensor you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4)\n",
    "Call the graph's `as_graph_def()` method and print the output. This is a protobuf representation of the computation graph: it is what makes TensorFlow models so portable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5)\n",
    "Get the concrete function's `function_def`, and look at its `signature`. This shows the names and types of the nodes in the graph that correspond to the function's inputs and outputs. This will come in handy when you deploy models to TensorFlow Serving or Google Cloud ML Engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Exercise solution](https://camo.githubusercontent.com/250388fde3fac9135ead9471733ee28e049f7a37/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f302f30362f46696c6f735f736567756e646f5f6c6f676f5f253238666c69707065642532392e6a7067)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 – Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1)\n",
    "Examine the code examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2)\n",
    "The function's graph is represented on the following diagram. Call the graph's `get_operations()` method to get the list of operations. Each operation has an `inputs` attribute that returns an iterator over its input tensors (these are symbolic: contrary to tensors we have used up to now, they have no value). It also has an `outputs` attribute that returns the list of output tensors. Each tensor has an `op` attribute that returns the operation it comes from. Try navigating through the graph using these methods and attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/cube_graph.png\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Operation 'x' type=Placeholder>,\n",
       " <tf.Operation 'Pow/y' type=Const>,\n",
       " <tf.Operation 'Pow' type=Pow>,\n",
       " <tf.Operation 'Identity' type=Identity>]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cube_func_int32.graph.get_operations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Operation 'Pow' type=Pow>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pow_op = cube_func_int32.graph.get_operations()[2]\n",
    "pow_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'x:0' shape=(None,) dtype=int32>,\n",
       " <tf.Tensor 'Pow/y:0' shape=() dtype=int32>]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pow_in = list(pow_op.inputs)\n",
    "pow_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'Pow:0' shape=(None,) dtype=int32>]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pow_out = list(pow_op.outputs)\n",
    "pow_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'x:0' shape=(None,) dtype=int32>,\n",
       " <tf.Tensor 'Pow/y:0' shape=() dtype=int32>]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pow_in = list(pow_op.inputs)\n",
    "pow_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Operation 'x' type=Placeholder>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pow_in[0].op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3)\n",
    "Each operation has a default name, such as `\"pow\"` (you can override it by setting the `name` attribute when you call the operation). In case of a name conflict, TensorFlow adds an underscore and anindex to make the name unique (e.g. `\"pow_1\"`). Moreover, each tensor has the same name as the operation that outputs it, followed by a colon `:` and the tensor's `index` (e.g., `\"pow:0\"`). Most operations have a single output tensor, so most tensors have a name that ends with `:0`. Try using `get_operation_by_name()` and `get_tensor_by_name()` to access any op and tensor you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Operation 'x' type=Placeholder>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cube_func_int32.graph.get_operation_by_name(\"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'x:0' shape=(None,) dtype=int32>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cube_func_int32.graph.get_tensor_by_name(\"x:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4)\n",
    "Call the graph's `as_graph_def()` method and print the output. This is a protobuf representation of the computation graph: it is what makes TensorFlow models so portable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "node {\n",
       "  name: \"x\"\n",
       "  op: \"Placeholder\"\n",
       "  attr {\n",
       "    key: \"_user_specified_name\"\n",
       "    value {\n",
       "      s: \"x\"\n",
       "    }\n",
       "  }\n",
       "  attr {\n",
       "    key: \"dtype\"\n",
       "    value {\n",
       "      type: DT_INT32\n",
       "    }\n",
       "  }\n",
       "  attr {\n",
       "    key: \"shape\"\n",
       "    value {\n",
       "      shape {\n",
       "        dim {\n",
       "          size: -1\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"Pow/y\"\n",
       "  op: \"Const\"\n",
       "  attr {\n",
       "    key: \"dtype\"\n",
       "    value {\n",
       "      type: DT_INT32\n",
       "    }\n",
       "  }\n",
       "  attr {\n",
       "    key: \"value\"\n",
       "    value {\n",
       "      tensor {\n",
       "        dtype: DT_INT32\n",
       "        tensor_shape {\n",
       "        }\n",
       "        int_val: 3\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"Pow\"\n",
       "  op: \"Pow\"\n",
       "  input: \"x\"\n",
       "  input: \"Pow/y\"\n",
       "  attr {\n",
       "    key: \"T\"\n",
       "    value {\n",
       "      type: DT_INT32\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"Identity\"\n",
       "  op: \"Identity\"\n",
       "  input: \"Pow\"\n",
       "  attr {\n",
       "    key: \"T\"\n",
       "    value {\n",
       "      type: DT_INT32\n",
       "    }\n",
       "  }\n",
       "}\n",
       "versions {\n",
       "  producer: 27\n",
       "}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cube_func_int32.graph.as_graph_def()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5)\n",
    "Get the concrete function's `function_def`, and look at its `signature`. This shows the names and types of the nodes in the graph that correspond to the function's inputs and outputs. This will come in handy when you deploy models to TensorFlow Serving or Google Cloud ML Engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"__inference_cube_402009\"\n",
       "input_arg {\n",
       "  name: \"x\"\n",
       "  type: DT_INT32\n",
       "}\n",
       "output_arg {\n",
       "  name: \"identity\"\n",
       "  type: DT_INT32\n",
       "}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cube_func_int32.function_def.signature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Exercise](https://c1.staticflickr.com/9/8101/8553474140_c50cf08708_b.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 – Autodiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1)\n",
    "Examine and run the following code examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 3. * x ** 2 + 2. * x - 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate_derivative(f, x, eps=1e-3):\n",
    "    return (f(x + eps) - f(x - eps)) / (2. * eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.999999999999119"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "approximate_derivative(f, 1.0) # true derivative = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xs = np.linspace(-2, 2, 200)\n",
    "fs = f(xs)\n",
    "x0 = 0.5\n",
    "df_x0 = approximate_derivative(f, x0)\n",
    "tangent_x0 = df_x0 * (xs - x0) + f(x0)\n",
    "plt.plot([-2, 2], [0, 0], \"k-\", linewidth=1)\n",
    "plt.plot([0, 0], [-5, 15], \"k-\", linewidth=1)\n",
    "plt.plot(xs, fs)\n",
    "plt.plot(xs, tangent_x0, \"r--\")\n",
    "plt.plot(x0, f(x0), \"ro\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"x\", fontsize=14)\n",
    "plt.ylabel(\"f(x)\", fontsize=14, rotation=0)\n",
    "plt.axis([-2, 2, -5, 15])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x1, x2):\n",
    "    return (x1 + 5) * (x2 ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate_gradient(f, x1, x2, eps=1e-3):\n",
    "    df_x1 = approximate_derivative(lambda x: f(x, x2), x1, eps)\n",
    "    df_x2 = approximate_derivative(lambda x: f(x1, x), x2, eps)\n",
    "    return df_x1, df_x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8.999999999993236, 41.999999999994486)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "approximate_gradient(g, 2.0, 3.0) # true gradient = (9, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: id=1049630, shape=(), dtype=float32, numpy=9.0>,\n",
       " <tf.Tensor: id=1049642, shape=(), dtype=float32, numpy=42.0>]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = tf.Variable(2.0)\n",
    "x2 = tf.Variable(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = g(x1, x2)\n",
    "grads = tape.gradient(z, [x1, x2])\n",
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientTape.gradient can only be called once on non-persistent tapes.\n"
     ]
    }
   ],
   "source": [
    "x1 = tf.Variable(2.0)\n",
    "x2 = tf.Variable(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = g(x1, x2)\n",
    "\n",
    "dz_x1 = tape.gradient(z, x1)\n",
    "try:\n",
    "    dz_x2 = tape.gradient(z, x2)\n",
    "except RuntimeError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=1049728, shape=(), dtype=float32, numpy=9.0>,\n",
       " <tf.Tensor: id=1049767, shape=(), dtype=float32, numpy=42.0>)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = tf.Variable(2.0)\n",
    "x2 = tf.Variable(3.0)\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = g(x1, x2)\n",
    "\n",
    "dz_x1 = tape.gradient(z, x1)\n",
    "dz_x2 = tape.gradient(z, x2)\n",
    "del tape\n",
    "dz_x1, dz_x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = tf.constant(2.0) # <= not Variable\n",
    "x2 = tf.constant(3.0) # <= not Variable\n",
    "with tf.GradientTape() as tape:\n",
    "    z = g(x1, x2)\n",
    "\n",
    "grads = tape.gradient(z, [x1, x2])\n",
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: id=402203, shape=(), dtype=float32, numpy=9.0>,\n",
       " <tf.Tensor: id=402215, shape=(), dtype=float32, numpy=42.0>]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = tf.constant(2.0)\n",
    "x2 = tf.constant(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x1)\n",
    "    tape.watch(x2)\n",
    "    z = g(x1, x2)\n",
    "\n",
    "grads = tape.gradient(z, [x1, x2])\n",
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=402272, shape=(), dtype=float32, numpy=13.0>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable(5.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    z1 = 3 * x\n",
    "    z2 = x ** 2\n",
    "tape.gradient([z1, z2], x) # dz1_x + dz2_x = 3 + 2x = 3 + 2*5 = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[None, <tf.Tensor: id=402335, shape=(), dtype=float32, numpy=6.0>],\n",
       " [<tf.Tensor: id=402389, shape=(), dtype=float32, numpy=6.0>,\n",
       "  <tf.Tensor: id=402372, shape=(), dtype=float32, numpy=14.0>]]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = tf.Variable(2.0)\n",
    "x2 = tf.Variable(3.0)\n",
    "with tf.GradientTape(persistent=True) as hessian_tape:\n",
    "    with tf.GradientTape() as jacobian_tape:\n",
    "        z = g(x1, x2)\n",
    "    jacobians = jacobian_tape.gradient(z, [x1, x2])\n",
    "hessians = [hessian_tape.gradient(jacobian, [x1, x2])\n",
    "            for jacobian in jacobians]\n",
    "del hessian_tape\n",
    "hessians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2)\n",
    "Implement Gradient Descent manually to find the value of `x` that minimizes the following function `f(x)`.\n",
    "\n",
    "**Tips**:\n",
    "* Define a variable `x` and initialize it to 0.\n",
    "* Define the `learning_rate` (e.g., 0.1).\n",
    "* Write a loop that will repeatedly (1) compute the gradient of `f` (actually a derivative in this case) at the current value of `x`, and (2) tweak `x` slightly in the opposite direction (by subtracting `learning_rate * df_dx`). You can use `x.assign_sub(...)` for this.\n",
    "* Using calculus, we can find that the algorithm should converge to $x = -\\frac{1}{3}$. Indeed, the derivative of $f(x) = 3 x^2 + 2x -1$ is $f'(x) = 6x + 2$, so the minimum is reached when $f'(x) = 0$ (slope is 0), so $6x + 2 = 0$, which leads to $x = -\\frac{1}{3}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 3. * x ** 2 + 2. * x - 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3)\n",
    "Now use an `SGD` optimizer instead of manually tweaking `x`.\n",
    "\n",
    "**Tips**:\n",
    "* You first need to create an `SGD` optimizer, optionally specifying the learning_rate (e.g., `lr=0.1`).\n",
    "* Next replace the manual tweaking of `x` in your previous code to use `optimizer.apply_gradients()` instead. You need to pass it a list of gradient/variable pairs (just one pair in this example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4)\n",
    "Create a `Sequential` model for the California housing problem (no need to compile it), and train it using your own training loop, instead of using `fit()`. Evaluate your model on the validation set at the end of each epoch, and display the result.\n",
    "\n",
    "**Tips**:\n",
    "* You can use the following `random_batch()` function to get a new batch of training data at each iteration (the Data API would be much preferable, as we will see in the next notebook).\n",
    "* You can use the model like a function to make predictions: `y_pred = model(X_batch)`\n",
    "* You can use `keras.losses.mean_squared_error()` to compute the loss. Note that it returns one loss per instance, so you need to use `tf.reduce_mean()` to get the mean loss. \n",
    "* You can use `model.trainable_variables` to get the full list of trainable variables in your model.\n",
    "* You can use `zip(gradients, variables)` to create a list containing all the gradient/variable pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(X, y, batch_size = 32):\n",
    "    idx = np.random.randint(0, len(X), size=batch_size)\n",
    "    return X[idx], y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5)\n",
    "Examine and run the following code examples, then update your training loop to display the training loss at each iteration.\n",
    "\n",
    "**Tips**:\n",
    "* You can use a `keras.metrics.MeanSquaredError` instance to efficiently track the running mean squared error at each iteration.\n",
    "* Make sure you reset the metric's states at the start of each epoch.\n",
    "* You can use `print(\"\\r\", mse, end=\"\")` to display the MSE on the same line at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=402433, shape=(), dtype=float32, numpy=5.0>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = keras.metrics.MeanSquaredError()\n",
    "metric([5.], [2.])  # error = (2 - 5)**2 = 9\n",
    "metric([0.], [1.])  # error = (1 - 0)**2 = 1\n",
    "metric.result()     # mean error = (9 + 1) / 2 = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=402439, shape=(), dtype=float32, numpy=0.0>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.reset_states()\n",
    "metric.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=402455, shape=(), dtype=float32, numpy=4.0>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric([1.], [3.])  # error = (3 - 1)**2 = 4\n",
    "metric.result()     # mean error = 4 / 1 = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Exercise solution](https://camo.githubusercontent.com/250388fde3fac9135ead9471733ee28e049f7a37/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f302f30362f46696c6f735f736567756e646f5f6c6f676f5f253238666c69707065642532392e6a7067)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 – Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1)\n",
    "Examine the code examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2)\n",
    "Implement Gradient Descent manually to find the value of `x` that minimizes the following function `f(x)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 3. * x ** 2 + 2. * x - 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "x = tf.Variable(0.0)\n",
    "\n",
    "for iteration in range(100):\n",
    "    with tf.GradientTape() as tape:\n",
    "        z = f(x)\n",
    "    dz_dx = tape.gradient(z, x)\n",
    "    x.assign_sub(learning_rate * dz_dx)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3)\n",
    "Now use an `SGD` optimizer instead of manually tweaking `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable(0.0)\n",
    "optimizer = keras.optimizers.SGD(lr=0.1)\n",
    "\n",
    "for iteration in range(100):\n",
    "    with tf.GradientTape() as tape:\n",
    "        z = f(x)\n",
    "    dz_dx = tape.gradient(z, x)\n",
    "    optimizer.apply_gradients([(dz_dx, x)])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4)\n",
    "Create a `Sequential` model for the California housing problem (no need to compile it), and train it using your own training loop, instead of using `fit()`. Evaluate your model on the validation set at the end of each epoch, and display the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(X, y, batch_size = 32):\n",
    "    idx = np.random.randint(0, len(X), size=batch_size)\n",
    "    return X[idx], y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 valid mse: 4.567194\n",
      "Epoch 1 valid mse: 1.0170918\n",
      "Epoch 2 valid mse: 1.7955109\n",
      "Epoch 3 valid mse: 0.596315\n",
      "Epoch 4 valid mse: 0.58101755\n",
      "Epoch 5 valid mse: 0.53072745\n",
      "Epoch 6 valid mse: 0.66618234\n",
      "Epoch 7 valid mse: 1.0245769\n",
      "Epoch 8 valid mse: 0.4872077\n",
      "Epoch 9 valid mse: 0.53296995\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 32\n",
    "steps_per_epoch = len(X_train) // batch_size\n",
    "optimizer = keras.optimizers.SGD()\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for step in range(steps_per_epoch):\n",
    "        X_batch, y_batch = random_batch(X_train_scaled, y_train, batch_size)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch)\n",
    "            loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "        grads = tape.gradient(loss, model.variables)\n",
    "        grads_and_vars = zip(grads, model.variables)\n",
    "        optimizer.apply_gradients(grads_and_vars)\n",
    "    y_pred = model(X_valid_scaled)\n",
    "    valid_loss = tf.reduce_mean(loss_fn(y_valid, y_pred))\n",
    "    print(\"Epoch\", epoch, \"valid mse:\", valid_loss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5)\n",
    "Examine and run the following code examples, then update your training loop to display the training loss at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=639680, shape=(), dtype=float32, numpy=5.0>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = keras.metrics.MeanSquaredError()\n",
    "metric([5.], [2.])  # error = (2 - 5)**2 = 9\n",
    "metric([0.], [1.])  # error = (1 - 0)**2 = 1\n",
    "metric.result()     # mean error = (9 + 1) / 2 = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=639686, shape=(), dtype=float32, numpy=0.0>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.reset_states()\n",
    "metric.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=639702, shape=(), dtype=float32, numpy=4.0>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric([1.], [3.])  # error = (3 - 1)**2 = 4\n",
    "metric.result()     # mean error = 4 / 1 = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  train mse: 2.7752223\tvalid mse: 5.3992133\n",
      "Epoch 1  train mse: 0.90346783\tvalid mse: 0.85499036\n",
      "Epoch 2  train mse: 0.73343664\tvalid mse: 0.65248495\n",
      "Epoch 3  train mse: 0.6572704\tvalid mse: 0.73229337\n",
      "Epoch 4  train mse: 0.6271633\tvalid mse: 0.89375675\n",
      "Epoch 5  train mse: 0.5680934\tvalid mse: 0.62792087\n",
      "Epoch 6  train mse: 0.5731681\tvalid mse: 0.548600125\n",
      "Epoch 7  train mse: 0.5597209\tvalid mse: 0.51854014\n",
      "Epoch 8  train mse: 0.52296084\tvalid mse: 0.5335406\n",
      "Epoch 9  train mse: 0.5074372\tvalid mse: 0.5076812\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 32\n",
    "steps_per_epoch = len(X_train) // batch_size\n",
    "optimizer = keras.optimizers.SGD()\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "metric = keras.metrics.MeanSquaredError()  # ADDED\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    metric.reset_states()  # ADDED\n",
    "    for step in range(steps_per_epoch):\n",
    "        X_batch, y_batch = random_batch(X_train_scaled, y_train, batch_size)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch)\n",
    "            loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            metric(y_batch, y_pred)  # ADDED\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        grads_and_vars = zip(grads, model.trainable_variables)\n",
    "        optimizer.apply_gradients(grads_and_vars)\n",
    "        print(\"\\rEpoch\", epoch, \" train mse:\", metric.result().numpy(), end=\"\")  # ADDED\n",
    "    y_pred = model(X_valid_scaled)\n",
    "    valid_loss = tf.reduce_mean(loss_fn(y_valid, y_pred))\n",
    "    print(\"\\tvalid mse:\", valid_loss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! You now know how to use TensorFlow's low-level API to write custom loss functions, layers, and models. You also learned how to optimize your functions by converting them to graphs: this allows TensorFlow to run operations in parallel and to perform various optimizations. Next, you learned how TensorFlow Functions and graphs are structured, and how to navigate through them. Finally, you learned how to use autodiff and write your own custom training loops."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
